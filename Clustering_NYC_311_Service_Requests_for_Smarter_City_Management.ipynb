{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x1scB-d2CFN"
      },
      "source": [
        "**Github:** ___________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvl7WbAgdMby"
      },
      "source": [
        "# **Clustering NYC 311 Service Requests for Smarter City Management**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIH9kKI92HIG"
      },
      "source": [
        "**Team 1**: Aastha Surana, Atishay Jain, Kendall Sims, Soham Sarvade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEk6nfiFw8b2"
      },
      "source": [
        "# **Problem Statement**\n",
        "\n",
        "NYC 311 receives thousands of service requests daily, but identifying patterns in complaints, response times, and agency involvement remains a challenge. Spatial and temporal trends are not well understood, making resource allocation inefficient. Additionally, extracting insights from resolution descriptions can help assess complaint handling and resident satisfaction. This project leverages association rule mining, clustering, and text analysis to uncover hidden patterns and enhance NYC 311 services.\n",
        "\n",
        "* **Community Issue Segmentation** – What are the most common complaints in different neighborhoods? Are certain issues more prevalent in specific areas?\n",
        "* **Temporal Trends** – How do service requests change over time? Are there seasonal spikes, daily peak hours, or other noticeable trends?\n",
        "* **Neighborhood Clustering** – Can we group neighborhoods based on the types and frequency of complaints? Identifying clusters could help streamline city services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie2Rlbnnzu3E"
      },
      "source": [
        "# **Motivation**\n",
        "\n",
        "New York City’s 311 service request system received over 42 million inquiries in 2018 ([NYC Open Data, 2019](https://https://council.nyc.gov/data/311-services/)), covering noise, sanitation, infrastructure, and more. However, due to the sheer volume of requests, identifying and handling recurring issues and emerging concerns remains a challenge for city agencies. Our project leverages unsupervised machine learning techniques—such as clustering and topic modeling—to extract meaningful insights that help stakeholders like city officials proactively address problems of the local people and improve urban management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nAtktNl0Njo"
      },
      "source": [
        "# **Business Relevance**\n",
        "Our findings will have wide-reaching implications:\n",
        "City Governance: Optimized resource allocation, improved response times, and data-driven policy decisions\n",
        "Urban Development: Identifies high-complaint zones, influencing real estate trends and infrastructure planning\n",
        "Private Sector & Utilities: Businesses can assess location-based risks, plan operations, and improve customer service strategies\n",
        "With predictive analytics reducing inefficiencies by 15-20% ([Harvard Data-Smart City Solutions, 2022](https://www.google.com/url?q=https://datasmart.hks.harvard.edu/news/article/how-can-data-and-analytics-be-used-to-enhance-city-operations-723&sa=D&source=docs&ust=1739680383860911&usg=AOvVaw2oqljbuUOOY1i8aZ3vuR3u)), our project will enhance public services and urban resilience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mughr9eAejwu"
      },
      "source": [
        "# **Executive Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6XhPTSAe__e"
      },
      "source": [
        "Our project Clustering NYC 311 Service Requests for Smarter City Management, aims to uncover key patterns within New York City’s (NYC) 311 service request data. By leveraging advanced data analysis techniques, including Exploratory Data Analysis (EDA), preprocessing, association rule mining, Principal Component Analysis (PCA), dimensionality reduction, K-means clustering and text mining, our goal is to assist city officials and businesses in making informed decisions regarding resource allocation and service optimization.\n",
        "\n",
        "**Objectives**\n",
        "\n",
        "* Community Issue Segmentation: Utilizing the Apriori algorithm to discover relationships between different types of complaints based on boroughs and cities, along with other factors.\n",
        "\n",
        "* Temporal Trends: Analyzing how service requests vary over time, including potential seasonal spikes and daily peak hours.\n",
        "\n",
        "* Neighborhood Clustering: Grouping neighborhoods based on the types and frequency of complaints to streamline city services.\n",
        "\n",
        "* Emerging Issues: Detecting new concerns that may require proactive intervention, helping to address problems before they escalate.\n",
        "\n",
        "**Findings**\n",
        "\n",
        "Association Rule Mining helped uncover hidden relationships between different complaint types against time and location. For instance, we discovered that noise complaints are more than usual on weekends, especially in Brooklyn and New York. As per another association rule, we see that NYPD is responsible for noise complaints. So, with this information, city officials can decide to allocate more resources or create response strategies for weekends in these specific areas to ensure efficacy.\n",
        "\n",
        "**Recommendations**\n",
        "\n",
        "The city officials can use the insights from association rules, clustering analysis and text mining to prioritize and allocate resources effectively, ensuring that neighborhoods with high complaint frequencies receive timely attention. Utilizing temporal trends to optimize staffing and operational strategies during peak service request times is also a good proactive approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVFu6nQo0d6R"
      },
      "source": [
        "# **Related Work**\n",
        "\n",
        "Data-driven governance is gaining traction worldwide. Studies show that:\n",
        "Smart city initiatives leveraging AI can enhance city services by up to 30% ([McKinsey & Company, 2018](https://www.mckinsey.com/capabilities/operations/our-insights/smart-cities-digital-solutions-for-a-more-livable-future))\n",
        "NYC’s 311 data has been analyzed for crime prediction and public health tracking ([National Library of Medicine, 2020](https://pmc.ncbi.nlm.nih.gov/articles/PMC7347128/))\n",
        "Our approach advances this field by applying cutting-edge clustering techniques to uncover hidden urban patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Telkmk1kc0xb"
      },
      "source": [
        "# **Dataset**\n",
        "\n",
        "**Data source:** [NYC OpenData](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/about_data)\n",
        "\n",
        "**Dataset Size and Features**: The dataset consists of over 28 million records and 41 features. Each record corresponds to a single 311 complaint, and the dataset includes categorical, numerical, textual, and geographic data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmyHxbAkxjzz"
      },
      "source": [
        "**Columns Description**\n",
        "\n",
        "Here is a brief description of the columns:\n",
        "\n",
        "| Column Title                     | Description |\n",
        "| :-------------------------------- | :----------------------------------------------------------- |\n",
        "| `unique_key`                      | Unique identifier of a Service Request (SR) in the open data set |\n",
        "| `created_date`                    | Date SR was created |\n",
        "| `closed_date`                     | Date SR was closed by responding agency |\n",
        "| `agency`                          | Acronym of responding City Government Agency |\n",
        "| `agency_name`                     | Full Agency name of responding City Government Agency |\n",
        "| `complaint_type`                  | The first level of a hierarchy identifying the topic of the incident or condition. May have a corresponding Descriptor. |\n",
        "| `descriptor`                      | Further detail on the incident or condition, dependent on Complaint Type. |\n",
        "| `location_type`                   | Describes the type of location used in the address information |\n",
        "| `incident_zip`                    | Incident location zip code, provided by geo validation. |\n",
        "| `incident_address`                | House number of incident address provided by submitter. |\n",
        "| `street_name`                     | Street name of incident address provided by the submitter |\n",
        "| `cross_street_1`                  | First Cross street based on the geo validated incident location |\n",
        "| `cross_street_2`                  | Second Cross Street based on the geo validated incident location |\n",
        "| `intersection_street_1`           | First intersecting street based on geo validated incident location |\n",
        "| `intersection_street_2`           | Second intersecting street based on geo validated incident location |\n",
        "| `address_type`                    | Type of incident location information available. |\n",
        "| `city`                             | City of the incident location provided by geovalidation. |\n",
        "| `landmark`                         | If the incident location is identified as a Landmark, the name of the landmark will display here |\n",
        "| `facility_type`                    | If available, describes the type of city facility associated with the SR |\n",
        "| `status`                           | Status of SR submitted |\n",
        "| `due_date`                         | Date when responding agency is expected to update the SR. Based on Complaint Type and internal SLAs. |\n",
        "| `resolution_description`           | Describes the last action taken on the SR by the responding agency. May describe next or future steps. |\n",
        "| `resolution_action_updated_date`   | Date when responding agency last updated the SR. |\n",
        "| `community_board`                  | Provided by geovalidation. |\n",
        "| `bbl`                              | Borough Block and Lot, provided by geovalidation. Identifies the location of buildings and properties in NYC. |\n",
        "| `borough`                          | Provided by the submitter and confirmed by geovalidation. |\n",
        "| `x_coordinate_state_plane`         | Geo validated, X coordinate of the incident location. |\n",
        "| `y_coordinate_state_plane`         | Geo validated, Y coordinate of the incident location. |\n",
        "| `open_data_channel_type`           | Indicates how the SR was submitted to 311 (e.g., Phone, Online, Mobile, Other, Unknown). |\n",
        "| `park_facility_name`               | If the incident location is a Parks Dept facility, the Name of the facility will appear here |\n",
        "| `park_borough`                     | The borough of the incident if it is a Parks Dept facility |\n",
        "| `vehicle_type`                     | If the incident is a taxi, describes the type of TLC vehicle. |\n",
        "| `taxi_company_borough`             | If the incident is identified as a taxi, displays the borough of the taxi company. |\n",
        "| `taxi_pick_up_location`            | If the incident is identified as a taxi, displays the taxi pick-up location |\n",
        "| `bridge_highway_name`              | If the incident is identified as a Bridge/Highway, the name will be displayed here. |\n",
        "| `bridge_highway_direction`         | If the incident is identified as a Bridge/Highway, displays the direction where the issue took place. |\n",
        "| `road_ramp`                        | If the incident location was Bridge/Highway, differentiates if the issue was on the Road or the Ramp. |\n",
        "| `bridge_highway_segment`           | Additional information on the section of the Bridge/Highway where the incident took place. |\n",
        "| `latitude`                         | Geo-based latitude of the incident location |\n",
        "| `longitude`                        | Geo-based longitude of the incident location |\n",
        "| `location`                         | Combination of the geo-based latitude & longitude of the incident location |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTqI5K_38GtF"
      },
      "source": [
        "**Sampling**\n",
        "\n",
        "We selected 1% of the total dataset (80,000 records) using stratified sampling, ensuring proportional representation from each year. This approach maintains data distribution while optimizing computational efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZXVODEnca4V"
      },
      "source": [
        "**Download and Explore the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjZ1m_y9ZR3Q",
        "outputId": "49360391-fa90-467b-9824-323668ed89f9"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-40187c28d59d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from IPython.display import display\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Evrtn6H8Zl9n"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "df = pd.read_csv('/content/drive/MyDrive/BA820_Team1/Data/311_sample_1percent.csv', on_bad_lines='skip')\n",
        "pd.set_option('display.max_columns', None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D09aaa6sspd_"
      },
      "outputs": [],
      "source": [
        "df_textmining = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KlmIygUCemmn"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "df_info = pd.DataFrame({\n",
        "    'Column Name': df.columns,\n",
        "    'Non-Null Count': df.notnull().sum(),\n",
        "    'Null Count': df.isnull().sum(),\n",
        "    'Unique Values': df.nunique(),\n",
        "    'Data Type': df.dtypes\n",
        "})\n",
        "\n",
        "df_info.reset_index(drop=True, inplace=True)\n",
        "\n",
        "from IPython.display import display\n",
        "display(df_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh9rVy_MpEhS"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0BolstTQrtF"
      },
      "source": [
        "In this project, EDA will help uncover trends, anomalies, and patterns in NYC’s 311 service requests. We analyze spatial, temporal, and categorical features to guide preprocessing, feature engineering, and clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cvfjtrSsYWh"
      },
      "source": [
        "**Pairplot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nh24FJkEn0dw"
      },
      "outputs": [],
      "source": [
        "numerical_cols = ['latitude', 'longitude', 'x_coordinate', 'y_coordinate', 'bbl', 'year', 'year_count']\n",
        "df_cleaned_1 = df[numerical_cols].dropna()\n",
        "sns.pairplot(df_cleaned_1, kind=\"hist\", corner=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adba2fv3sFmA"
      },
      "source": [
        "Based on the pairplot, we observe:\n",
        "\n",
        "1. **No Strong Correlation:** The scatterplots do not show clear linear relationships between most variables, indicating weak or no correlation among them. This suggests that individual features may not be predictive of each other.\n",
        "\n",
        "2. **Possible Clustering:** Some feature pairs exhibit slight grouping tendencies, hinting that certain types of 311 service requests might cluster together based on specific attributes like location or complaint type.\n",
        "\n",
        "3. **Varying Distributions:** The density of points varies across different feature combinations, indicating that some service request categories might be more frequent or concentrated in specific conditions than others.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBZKUYSAQwZm"
      },
      "source": [
        "**Complaints by Year**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rO7GFTGSQy_O"
      },
      "outputs": [],
      "source": [
        "# Ensure created_date is in datetime format\n",
        "df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Number of Complaints Each Year\n",
        "df['year'] = df['created_date'].dt.year\n",
        "yearly_complaints = df['year'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=yearly_complaints.index, y=yearly_complaints.values, color=\"royalblue\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Complaints\")\n",
        "plt.title(\"311 Service Requests by Year\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TliJVKAWQ2A-"
      },
      "source": [
        "- 311 complaints increased steadily until 2018, indicating growing reliance on the system.\n",
        "- The drop in 2019-2021 may be linked to pandemic-related behavioral changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvybWiuOQ6ge"
      },
      "source": [
        "**Complaints by Day of week**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YPZ7v1W9Q9At"
      },
      "outputs": [],
      "source": [
        "# Number of Complaints Each Day of the Week\n",
        "df['day_of_week'] = df['created_date'].dt.day_name()\n",
        "day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
        "weekly_complaints = df['day_of_week'].value_counts().reindex(day_order)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=weekly_complaints.index, y=weekly_complaints.values, color=\"tomato\")\n",
        "plt.xlabel(\"Day of the Week\")\n",
        "plt.ylabel(\"Number of Complaints\")\n",
        "plt.title(\"Total 311 Complaints by Day of the Week\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGNkhgZwQ_jX"
      },
      "source": [
        "- Service requests peak between Monday and Friday, with the highest volume on Tuesday.\n",
        "- Complaints are often filed during the workweek, potentially driven by business and city agency operations.\n",
        "- The drop on weekends suggests lower reporting activity rather than an actual decline in issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fksoQU48RGY2"
      },
      "source": [
        "**Complaints by Hour of the Day**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "km_B2XIqRInk"
      },
      "outputs": [],
      "source": [
        "# Number of Complaints by Hour of the Day\n",
        "df['hour_of_day'] = df['created_date'].dt.hour\n",
        "hourly_complaints = df['hour_of_day'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.barplot(x=hourly_complaints.index, y=hourly_complaints.values, color=\"seagreen\")\n",
        "plt.xlabel(\"Hour of the Day\")\n",
        "plt.ylabel(\"Number of Complaints\")\n",
        "plt.title(\"Total 311 Complaints by Hour of the Day\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gePhe63RKs6"
      },
      "source": [
        "- There is a sharp spike in complaints at midnight, likely due to noise complaints.\n",
        "- Complaint volume remains low in the early morning hours but steadily increases throughout the day.\n",
        "- Peak reporting occurs between late morning and early afternoon.\n",
        "- Understanding these patterns can help city agencies optimize response times and resource allocation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjuZi5fnRQHh"
      },
      "source": [
        "**Mode of Complaint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aSjfsOOeRSKg"
      },
      "outputs": [],
      "source": [
        "# Complaint Channel Type Distribution (Pie Chart)\n",
        "plt.figure(figsize=(7,7))\n",
        "channel_counts = df['open_data_channel_type'].value_counts()\n",
        "colors = [\"#00CED1\", \"#4682B4\", \"#5F9EA0\", \"#2F4F4F\", \"#20B2AA\"]  # Cyan, Steel Blue, Cadet Blue, Dark Slate Gray, Light Sea Green\n",
        "\n",
        "plt.pie(channel_counts, labels=channel_counts.index, autopct='%1.1f%%', colors=colors, startangle=140, wedgeprops={\"edgecolor\": \"white\"})\n",
        "plt.title(\"Complaint Distribution by Channel Type\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NdWLOMeRUhV"
      },
      "source": [
        "- The majority of complaints are reported via phone (49.1%), followed by online platforms (21.4%).\n",
        "- The high reliance on phone reporting suggests accessibility issues with digital alternatives.\n",
        "- Increasing mobile and online adoption could enhance efficiency and streamline service request processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4-PGisERbBI"
      },
      "source": [
        "**Top 10 Complaint Types**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PcvTu_k4RdMe"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "top_10_complaints = df['complaint_type'].value_counts().nlargest(10).index\n",
        "\n",
        "sns.countplot(y=df[df['complaint_type'].isin(top_10_complaints)]['complaint_type'],\n",
        "              order=top_10_complaints, color=\"#a9a9a9\")\n",
        "\n",
        "plt.xlabel(\"Number of Complaints\")\n",
        "plt.ylabel(\"Complaint Type\")\n",
        "plt.title(\"Top 10 Complaint Types\")\n",
        "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9m9EzUURfe2"
      },
      "source": [
        "- Noise complaints (residential and street) dominate the 311 system.\n",
        "- Other major concerns include heat/hot water issues, illegal parking, and blocked driveways.\n",
        "- These recurring issues highlight key urban challenges that demand targeted policy interventions.\n",
        "- High-density residential areas may require better noise control and heating regulations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQQ6RmSeRimj"
      },
      "source": [
        "**Top 10 Agencies Handling Complaints**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "galDDdNWRk0O"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "# Get top 10 agencies handling complaints\n",
        "top_10_agencies = df['agency_name'].value_counts().nlargest(10).index\n",
        "\n",
        "sns.countplot(y=df[df['agency_name'].isin(top_10_agencies)]['agency_name'],\n",
        "              order=top_10_agencies, color=\"#9370DB\")\n",
        "plt.xlabel(\"Number of Complaints\")\n",
        "plt.ylabel(\"Agency Name\")\n",
        "plt.title(\"Top 10 Agencies Handling Complaints\")\n",
        "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)  # Gridlines only on x-axis for readability\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn0Zh3WqRnDt"
      },
      "source": [
        "- The NYPD receives the highest volume of complaints, followed by Housing Preservation and Development (HPD) and the Department of Transportation.\n",
        "- The focus on law enforcement, housing, and infrastructure suggests these are the most pressing urban concerns.\n",
        "- Clustering complaints by agency could help improve inter-departmental coordination and efficiency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOXG1KyHRp1z"
      },
      "source": [
        "**Complaints by Borough**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lP8OElg7Rr6I"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x=df['borough'],\n",
        "              order=df['borough'].value_counts().index,\n",
        "              color=\"#DC143C\")\n",
        "plt.xlabel(\"Borough\")\n",
        "plt.ylabel(\"Number of Complaints\")\n",
        "plt.title(\"Complaints by Borough\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfEtTJEIRuPU"
      },
      "source": [
        "- Brooklyn and Queens generate the highest number of complaints, reflecting their large populations and infrastructure challenges.\n",
        "- Manhattan and the Bronx also report significant complaint volumes.\n",
        "- Staten Island and the \"Unspecified\" category have significantly lower volumes, which may indicate gaps in reporting or data collection discrepancies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va9Wl5dHRyPP"
      },
      "source": [
        "**Geospatial Analysis of Complaints**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TBNrZJ8sR0ev"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df['longitude'], y=df['latitude'],\n",
        "                alpha=0.07, color=\"blue\", s=1)\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.title(\"Geographical Distribution of Complaints\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g12KAny5R3E4"
      },
      "source": [
        "- Complaints are concentrated in densely populated areas, particularly in Manhattan and Brooklyn.\n",
        "- Spatial clustering can reveal high-complaint zones, helping city officials prioritize interventions.\n",
        "- Certain areas may require targeted policies or improved resource allocation to address persistent issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZc8NQ-FXbM5"
      },
      "source": [
        "**Complaints Over Time (Time Series Plot)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1vE7ScNNXdLw"
      },
      "outputs": [],
      "source": [
        "df['created_date'] = pd.to_datetime(df['created_date'])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "df.set_index('created_date').resample('M').size().plot()\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Complaints\")\n",
        "plt.title(\"Number of Complaints Over Time\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2HldF6FXfIb"
      },
      "source": [
        "- Complaint trends show fluctuations with periodic spikes, indicating seasonal variations or policy changes impacting service requests.\n",
        "- Identifying patterns in these fluctuations can help predict future demand for city services.\n",
        "- Predictive analytics could assist in proactive urban planning and response strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lasrF0E1XiDM"
      },
      "source": [
        "**Resolution Time**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HYZemWLmXkUu"
      },
      "outputs": [],
      "source": [
        "# Ensure datetime format\n",
        "df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')\n",
        "df['closed_date'] = pd.to_datetime(df['closed_date'], errors='coerce')\n",
        "\n",
        "# Drop rows where closed_date is null (i.e., unresolved requests)\n",
        "df = df.dropna(subset=['closed_date'])\n",
        "\n",
        "# Calculate resolution time in hours\n",
        "df['resolution_time'] = (df['closed_date'] - df['created_date']).dt.total_seconds() / 3600\n",
        "\n",
        "# Drop negative resolution times\n",
        "df = df[df['resolution_time'] >= 0]\n",
        "\n",
        "# Remove outliers using the Interquartile Range (IQR) method\n",
        "Q1, Q3 = df['resolution_time'].quantile([0.25, 0.75])\n",
        "IQR = Q3 - Q1\n",
        "lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "\n",
        "df = df[(df['resolution_time'] >= lower_bound) &\n",
        "                             (df['resolution_time'] <= upper_bound)]\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['resolution_time'], bins=50, kde=True, color='blue')\n",
        "\n",
        "# Adjust y-axis labels to be in '000\n",
        "plt.ylabel(\"Number of Complaints\")\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x/1_000)}K'))\n",
        "\n",
        "plt.xlabel(\"Resolution Time (Hours)\")\n",
        "plt.title(\"Complaint Resolution Time\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VloqJcETXmxg"
      },
      "source": [
        "- Most complaints are resolved within a short time, but a long tail in resolution times indicates inefficiencies in addressing certain cases.\n",
        "- Certain complaint types may take disproportionately longer to resolve, affecting resident satisfaction.\n",
        "- Understanding which complaints have prolonged resolution times can help improve service delivery and accountability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlaA-b_Ue0jT"
      },
      "source": [
        "# **Data Cleaning Process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euhNwNxRVB5y"
      },
      "source": [
        "**Step 1: Drop Unnecessary Columns**\n",
        "We remove columns that are **not useful for clustering or analysis**. These include:\n",
        "- **Redundant location fields** (e.g., `street_name`, `cross_street_1`) since we already have `incident_zip`, `borough`, `latitude`, and `longitude`.\n",
        "- **Unique identifiers** (`unique_key`) that do not contribute to pattern detection.\n",
        "- **Irrelevant categorical columns** (`agency`, `landmark`, `vehicle_type`, etc.) that add complexity but do not provide meaningful clustering information.\n",
        "- **Other miscellaneous fields** like `due_date`, `resolution_action_updated_date`, and `year_count`, which are unnecessary for the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iLaX4vbBw_rh"
      },
      "outputs": [],
      "source": [
        "# Step 1: Drop Unnecessary Columns\n",
        "all_columns = set(df.columns)\n",
        "drop_columns = {\n",
        "    'unique_key', 'incident_address', 'street_name', 'cross_street_1', 'cross_street_2',\n",
        "    'intersection_street_1', 'intersection_street_2', 'address_type', 'landmark', 'facility_type',\n",
        "    'due_date', 'resolution_action_updated_date', 'community_board', 'x_coordinate', 'y_coordinate',\n",
        "    'park_facility_name', 'park_borough', 'bbl', 'vehicle_type', 'taxi_company_borough',\n",
        "    'taxi_pickup_location', 'bridge_highway_name', 'bridge_highway_direction', 'road_ramp',\n",
        "    'bridge_highway_segment', 'location', 'year', 'row_num', 'year_count', 'created_hour', 'closed_hour'\n",
        "}\n",
        "existing_drop_columns = list(drop_columns.intersection(all_columns))\n",
        "df.drop(columns=existing_drop_columns, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SM-0iImWAFPm"
      },
      "outputs": [],
      "source": [
        "df_association_rules=df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpp0keBSxQEy"
      },
      "source": [
        "**Step 2: Preprocessing**\n",
        "\n",
        "**Extract Time Features**\n",
        "- Extracted `created_hour_of_day` from `created_date` to **analyze complaint patterns by hour**.\n",
        "- Extracted `day_of_week` from `created_date` to **identify trends in complaints by weekday**.\n",
        "- Converted `day_of_week` into a **numerical format** (`Monday = 1`, `Sunday = 7`) for better machine learning compatibility.\n",
        "\n",
        "**Format Date Columns**\n",
        "- Converted `created_date` and `closed_date` into **MM-DD-YYYY format** to remove the time component and standardize presentation.\n",
        "\n",
        "**Handle Missing Values**\n",
        "- Dropped rows where `closed_date` is missing since these records are incomplete and cannot be used for response time analysis.\n",
        "- Replaced missing values in categorical columns (`borough`, `status`, etc.) with the **most frequent value (mode)** to retain data consistency.\n",
        "\n",
        "**Standardize Text Data**\n",
        "- Converted text columns (`complaint_type`, `descriptor`, `location_type`, etc.) to **title case** for uniformity.\n",
        "- Replaced ambiguous values:\n",
        "  - `\"Other/Unknown\"` → `\"-\"`\n",
        "  - `\"See notes.\"` → `\"-\"`\n",
        "  - `\"Unspecified\"` → `\"-\"`\n",
        "  - `\"Unknown\"` → `\"-\"`\n",
        "\n",
        "**Convert Resolution Time**\n",
        "- Renamed `resolution_time` to **`resolution_time_minutes`** and converted it into **integer format** for numerical processing.\n",
        "\n",
        "**Validate and Clean ZIP Codes**\n",
        "- Filtered `incident_zip` to keep **only valid 5-digit ZIP codes**.\n",
        "- Converted `incident_zip` to an **integer** for proper numerical analysis.\n",
        "\n",
        "**Handle Missing Coordinates**\n",
        "- Imputed missing `latitude` and `longitude` values using their **mode** to maintain geospatial consistency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MSevo5a4xFad"
      },
      "outputs": [],
      "source": [
        "# Step 2: Preprocessing\n",
        "\n",
        "# Extract hour of day\n",
        "if 'created_date' in df.columns:\n",
        "    df.loc[:, 'created_hour_of_day'] = pd.to_datetime(df['created_date'], errors='coerce').dt.hour.astype('Int64')\n",
        "\n",
        "# Extract day_of_week before converting date format\n",
        "if 'created_date' in df.columns:\n",
        "    df.loc[:, 'day_of_week'] = pd.to_datetime(df['created_date'], errors='coerce').dt.day_name()\n",
        "\n",
        "# Convert day_of_week from text to numeric representation\n",
        "day_mapping = {\n",
        "    \"Monday\": 1, \"Tuesday\": 2, \"Wednesday\": 3, \"Thursday\": 4, \"Friday\": 5, \"Saturday\": 6, \"Sunday\": 7\n",
        "}\n",
        "df.loc[:, 'day_of_week'] = df['day_of_week'].map(day_mapping)\n",
        "\n",
        "# Convert created_date & closed_date to datetime format\n",
        "if 'created_date' in df.columns:\n",
        "    df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce').dt.strftime('%m-%d-%Y')\n",
        "if 'closed_date' in df.columns:\n",
        "    df['closed_date'] = pd.to_datetime(df['closed_date'], errors='coerce').dt.strftime('%m-%d-%Y')\n",
        "\n",
        "# Drop rows where closed_date is missing\n",
        "df = df.dropna(subset=['closed_date']).copy()\n",
        "\n",
        "# Convert text columns to title case\n",
        "title_case_cols = ['complaint_type', 'descriptor', 'location_type', 'city', 'borough', 'open_data_channel_type']\n",
        "existing_title_case_cols = [col for col in title_case_cols if col in df.columns]\n",
        "df.loc[:, existing_title_case_cols] = df[existing_title_case_cols].apply(lambda x: x.str.title() if x.dtype == 'object' else x)\n",
        "\n",
        "# Replace specific text values\n",
        "df.loc[df['descriptor'] == \"Other/Unknown\", 'descriptor'] = \"-\"\n",
        "df.loc[df['resolution_description'] == \"See notes.\", 'resolution_description'] = \"-\"\n",
        "df.loc[df['borough'] == \"Unspecified\", 'borough'] = \"-\"\n",
        "df.loc[df['open_data_channel_type'] == \"Unknown\", 'open_data_channel_type'] = \"-\"\n",
        "\n",
        "# Replace NaN or unknown values with mode for categorical columns\n",
        "fill_mode_cols = ['location_type', 'city', 'status', 'borough', 'open_data_channel_type', 'descriptor']\n",
        "existing_fill_mode_cols = [col for col in fill_mode_cols if col in df.columns]\n",
        "df.loc[:, existing_fill_mode_cols] = df[existing_fill_mode_cols].apply(lambda x: x.fillna(x.mode()[0]))\n",
        "\n",
        "# Convert resolution_time into an integer\n",
        "if 'resolution_time' in df.columns:\n",
        "    df.rename(columns={'resolution_time': 'resolution_time_minutes'}, inplace=True)\n",
        "    df.loc[:, 'resolution_time_minutes'] = df['resolution_time_minutes'].astype(int)\n",
        "\n",
        "# Filter valid ZIP codes (keep only 5-digit ZIP codes)\n",
        "if 'incident_zip' in df.columns:\n",
        "    df = df[df['incident_zip'].astype(str).str.match(r'^[0-9]{5}$', na=False)].copy()\n",
        "    df.loc[:, 'incident_zip'] = df['incident_zip'].astype(int)\n",
        "\n",
        "# Impute missing latitude and longitude with mode\n",
        "if 'latitude' in df.columns and 'longitude' in df.columns:\n",
        "        df.loc[:, 'latitude'] = df['latitude'].fillna(df['latitude'].mode()[0])\n",
        "        df.loc[:, 'longitude'] = df['longitude'].fillna(df['longitude'].mode()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU_VEEsbxZ_Z"
      },
      "source": [
        "**Step 3: Enforcing Data Types**\n",
        "- Converted `incident_zip` to **integer** for better indexing.\n",
        "- Converted `latitude` and `longitude` to **float** for accurate geospatial processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0UGcyjDXxHuq"
      },
      "outputs": [],
      "source": [
        "# Step 3: Enforcing Data Types\n",
        "if 'incident_zip' in df.columns:\n",
        "    df.loc[:, 'incident_zip'] = df['incident_zip'].astype(int)\n",
        "if 'latitude' in df.columns:\n",
        "    df.loc[:, 'latitude'] = df['latitude'].astype(float)\n",
        "if 'longitude' in df.columns:\n",
        "    df.loc[:, 'longitude'] = df['longitude'].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3tkUVh6xclZ"
      },
      "source": [
        "**Step 4: Reordering Columns**\n",
        "The dataset is reorganized for **better readability**:\n",
        "1. **Date & Time Information**: `created_date`, `closed_date`, `created_hour_of_day`, `day_of_week`, `resolution_time_minutes`\n",
        "2. **Agency & Location Details**: `agency_name`, `incident_zip`, `borough`, `city`, `latitude`, `longitude`\n",
        "3. **Complaint Information**: `complaint_type`, `descriptor`, `resolution_description`, `status`, `location_type`, `open_data_channel_type`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n4SwywpybG8u"
      },
      "outputs": [],
      "source": [
        "# Step 4: Reordering Columns\n",
        "column_order = [\n",
        "    'created_date', 'closed_date', 'created_hour_of_day', 'day_of_week', 'resolution_time_minutes', 'agency_name',\n",
        "    'incident_zip', 'borough', 'city', 'latitude', 'longitude',\n",
        "    'complaint_type', 'descriptor', 'resolution_description', 'status', 'location_type', 'open_data_channel_type'\n",
        "]\n",
        "df = df[[col for col in column_order if col in df.columns]]\n",
        "\n",
        "df.reset_index(drop=True, inplace=True) # Reset index for better processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cDxa3v6WJFG7"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7qS3ARjgKcEr"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "df_info = pd.DataFrame({\n",
        "    'Column Name': df.columns,\n",
        "    'Non-Null Count': df.notnull().sum(),\n",
        "    'Null Count': df.isnull().sum(),\n",
        "    'Unique Values': df.nunique(),\n",
        "    'Data Type': df.dtypes\n",
        "})\n",
        "\n",
        "df_info.reset_index(drop=True, inplace=True)\n",
        "\n",
        "from IPython.display import display\n",
        "display(df_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K6pjVbHhH-qC"
      },
      "outputs": [],
      "source": [
        "# Look into why resolution time is zero for some cases\n",
        "resolution_zero = df[df['resolution_time_minutes'] == 0]\n",
        "resolution_zero.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udh5ARevHHRp"
      },
      "source": [
        "We noticed that some cases have a resolution time of zero, and we decided to look into why this might be occurring. For these special cases, the created date and closed date are the same, so this request was most likely resolved right away- no paperwork or follow-up was needed. For this reason, we decided to keep these rows in the dataset because it provides valuable insight into our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDBBmRhSs3rt"
      },
      "source": [
        "# **Featuring Numerical Columns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H8MGLQfoBXl"
      },
      "source": [
        "Our dataset primarily consists of categorical features with limited numerical data. Since most unsupervised learning algorithms require numerical inputs, and handling categorical data directly is beyond the scope of our course, we transformed key categorical variables into numerical representations. This feature engineering process enabled us to extract meaningful insights and enhance the effectiveness of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8l1WlnNyob9"
      },
      "source": [
        "**1. Zip Complaint Density**\n",
        "\n",
        "**Description** : Represents the proportion of total complaints from each ZIP code.\n",
        "\n",
        "**Why?** : Identifies areas with the highest complaint volumes.\n",
        "\n",
        "**How it helps?** : Supports neighborhood clustering and resource allocation by highlighting high-demand areas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QjA-N50azdb9"
      },
      "outputs": [],
      "source": [
        "# Zip Complaint Density\n",
        "if 'incident_zip' in df.columns:\n",
        "    zip_complaint_counts = df['incident_zip'].value_counts(normalize=True)\n",
        "    df['zip_complaint_density'] = df['incident_zip'].map(zip_complaint_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wzKCqzzf-2"
      },
      "source": [
        "**2. Complaint Type Score**\n",
        "\n",
        "**Description** : Measures the normalized frequency of each complaint type.\n",
        "\n",
        "**Why?** : Helps group neighborhoods based on recurring complaint types.\n",
        "\n",
        "**How it helps?** : Enables issue segmentation, helping officials prioritize common vs. rare service issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wshFAT7LzimT"
      },
      "outputs": [],
      "source": [
        "#Complaint Type Score\n",
        "if 'complaint_type' in df.columns:\n",
        "    complaint_type_counts = df['complaint_type'].value_counts(normalize=True)\n",
        "    df['complaint_type_score'] = df['complaint_type'].map(complaint_type_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKJIMWuGzliG"
      },
      "source": [
        "**3. Response Delay Hours**\n",
        "\n",
        "**Description** : Calculates the time difference (in hours) between the due date and the complaint resolution.\n",
        "\n",
        "**Why?** : Tracks how efficiently complaints are addressed.\n",
        "\n",
        "**How it helps?** : Reveals delays in service response, enabling better operational planning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VG-cEBxVznvA"
      },
      "outputs": [],
      "source": [
        "# Response Delay Hours\n",
        "if 'closed_date' in df.columns and 'due_date' in df.columns:\n",
        "    df['closed_date'] = pd.to_datetime(df['closed_date'], errors='coerce')\n",
        "    df['due_date'] = pd.to_datetime(df['due_date'], errors='coerce')\n",
        "    df['response_delay_hours'] = (df['closed_date'] - df['due_date']).dt.total_seconds() / 3600\n",
        "    df['response_delay_hours'] = df['response_delay_hours'].fillna(0)\n",
        "else:\n",
        "    df['response_delay_hours'] = 0  # Default value if 'due_date' is missing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR4tu_eEzqCo"
      },
      "source": [
        "**4. Borough Complaint Density**\n",
        "\n",
        "**Description** : Proportion of complaints originating from each borough.\n",
        "\n",
        "**Why?** : Helps analyze complaint distribution across different boroughs.\n",
        "\n",
        "**How it helps?** : Ensures equitable resource distribution and identifies boroughs requiring urgent intervention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vXwDSmDGzs7x"
      },
      "outputs": [],
      "source": [
        "#Borough Complaint Density\n",
        "if 'borough' in df.columns:\n",
        "    borough_complaint_counts = df['borough'].value_counts(normalize=True)\n",
        "    df['borough_complaint_density'] = df['borough'].map(borough_complaint_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23mt1E_FzvRV"
      },
      "source": [
        "**5. Resolution Time Minutes**\n",
        "\n",
        "**Description** : Measures the total time (in minutes) taken to resolve a complaint.\n",
        "\n",
        "**Why?** : Differentiates between quick and prolonged resolutions.\n",
        "\n",
        "**How it helps?** : Improves efficiency analysis, ensuring service delays are minimized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WuiWHGgKzxKX"
      },
      "outputs": [],
      "source": [
        "# Resolution Time Minutes\n",
        "if 'created_date' in df.columns and 'closed_date' in df.columns:\n",
        "    df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')\n",
        "    df['closed_date'] = pd.to_datetime(df['closed_date'], errors='coerce')\n",
        "\n",
        "    # Compute resolution time in minutes\n",
        "    df['resolution_time_minutes'] = (df['closed_date'] - df['created_date']).dt.total_seconds() / 60\n",
        "\n",
        "    # Handle missing values by filling with the median\n",
        "    df['resolution_time_minutes'] = df['resolution_time_minutes'].fillna(df['resolution_time_minutes'].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5ID3IUazyfP"
      },
      "source": [
        "**6. Weekend vs. Weekday Complaints Ratio**\n",
        "\n",
        "**Description** : Ratio of complaints reported on weekends vs. weekdays.\n",
        "\n",
        "**Why?** : Detects temporal patterns in complaint filings.\n",
        "\n",
        "**How it helps?** : Aids in staffing and resource planning, ensuring adequate service availability during peak hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gb_00_1_u2te"
      },
      "outputs": [],
      "source": [
        "# Weekend vs. Weekday Complaints Ratio\n",
        "if 'created_date' in df.columns:\n",
        "    df['day_of_week'] = df['created_date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "    weekend_complaints = df['day_of_week'].apply(lambda x: 1 if x in [5, 6] else 0)\n",
        "    df['weekend_vs_weekday_complaints'] = weekend_complaints.mean()  # Ratio of weekend complaints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z1nT7f5s05gk"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mld0GMVkwLbp"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "df_info = pd.DataFrame({\n",
        "    'Column Name': df.columns,\n",
        "    'Non-Null Count': df.notnull().sum(),\n",
        "    'Null Count': df.isnull().sum(),\n",
        "    'Unique Values': df.nunique(),\n",
        "    'Data Type': df.dtypes\n",
        "})\n",
        "\n",
        "df_info.reset_index(drop=True, inplace=True)  # Reset the index to avoid repetition\n",
        "\n",
        "from IPython.display import display\n",
        "display(df_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3HjwbvHun26"
      },
      "source": [
        "# **Preprocessing of Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DqXORXxr8gWG"
      },
      "outputs": [],
      "source": [
        "# Numerical columns (for clustering and numerical analysis)\n",
        "numerical_columns = [\n",
        "    'created_hour_of_day', 'day_of_week', 'resolution_time_minutes',\n",
        "    'latitude', 'longitude', 'zip_complaint_density', 'complaint_type_score',\n",
        "    'response_delay_hours', 'borough_complaint_density', 'weekend_vs_weekday_complaints'\n",
        "]\n",
        "\n",
        "# Categorical columns (for association rules and one-hot encoding)\n",
        "categorical_columns = [\n",
        "    'agency_name', 'incident_zip', 'borough', 'city', 'status',\n",
        "    'location_type', 'open_data_channel_type'\n",
        "]\n",
        "\n",
        "# Text columns (for NLP and text mining)\n",
        "text_columns = ['complaint_type', 'descriptor', 'resolution_description']\n",
        "\n",
        "# Print the column groups\n",
        "print(\"Numerical Columns:\", numerical_columns)\n",
        "print(\"Categorical Columns:\", categorical_columns)\n",
        "print(\"Text Columns:\", text_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XFSnnsZA9lIJ"
      },
      "outputs": [],
      "source": [
        "# Standardization & Scaling\n",
        "scaler = StandardScaler()\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OK-ePIJSSqTA"
      },
      "outputs": [],
      "source": [
        "# One-Hot Encoding for selected columns\n",
        "one_hot_cols = ['borough', 'status', 'open_data_channel_type']\n",
        "\n",
        "available_one_hot_cols = [col for col in one_hot_cols if col in df.columns]\n",
        "if available_one_hot_cols:\n",
        "    one_hot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "    encoded_cats = pd.DataFrame(one_hot_encoder.fit_transform(df[available_one_hot_cols]),\n",
        "                                columns=one_hot_encoder.get_feature_names_out(available_one_hot_cols))\n",
        "    df = df.drop(columns=available_one_hot_cols).join(encoded_cats)\n",
        "\n",
        "# Label Encoding for selected columns\n",
        "label_encode_cols = ['agency_name', 'incident_zip', 'city', 'location_type']\n",
        "\n",
        "available_label_cols = [col for col in label_encode_cols if col in df.columns]\n",
        "for col in available_label_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z2dNYK2kO3z3"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p41Nxe6VeFM"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbzkI551Kuir"
      },
      "source": [
        "## **1. Association Rule Mining**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfW1RwBiOIVf"
      },
      "source": [
        "**Goal**\n",
        "\n",
        "Under Association Rule Mining, we aim to derive meaningful insights from the NYC 311 dataset by exploring the relationships between various complaint types and geographical and temporal attributes like boroughs, agency, weekly trends and resolution time. We are looking forward to identify patterns that can inform city services and improve response strategies  \n",
        "\n",
        "**Approach Used**\n",
        "\n",
        "Our approach involves implementation of the Apriori algorithm to uncover frequent itemsets. We will then generate association rules to understand how different complaint types correlate with each other, considering factors like location, time, and complaint categories. This process will enable us to reveal significant associations, which can ultimately help in enhancing service efficiency and prioritizing resource allocation in the city.\n",
        "\n",
        "**Challenges**\n",
        "\n",
        "- Ensuring correct data types for categorical variables was essential for encoding.  \n",
        "- Many rules had low lift values close to 1, making them weak associations despite high confidence.  \n",
        "- Filtering meaningful consequents like status, resolution time, and agency was necessary to avoid irrelevant patterns.  \n",
        "- Transaction encoding required converting tuples into lists for proper processing.  \n",
        "- A high number of unique complaint types increased complexity in pattern detection.  \n",
        "- Some rules with high lift had very low support, making them statistically weak.  \n",
        "- Interpreting results required ensuring that only relevant consequents appeared to derive actionable insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YvbCzVDsGy73"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "categorical_cols = [\"complaint_type\", \"city\"]\n",
        "transactions = df_association_rules[categorical_cols].astype(str).values.tolist()\n",
        "\n",
        "te = TransactionEncoder()\n",
        "encoded_array = te.fit(transactions).transform(transactions)\n",
        "\n",
        "df_encoded = pd.DataFrame(encoded_array, columns=te.columns_)\n",
        "df_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FuoYK2D5IvaM"
      },
      "outputs": [],
      "source": [
        "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
        "frequent_itemsets.sort_values(by=\"support\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79OlUK49I6WX"
      },
      "source": [
        "**Frequent Itemsets**\n",
        "\n",
        "* Most Common Complaint Types – The most frequently reported complaints include \"Noise - Residential\", \"Heat/Hot Water\" and \"Illegal Parking.\"\n",
        "\n",
        "* Frequent Cities in Complaints – Brooklyn has the highest share of complaints at 35.5%, followed by New York (18.8%) and Bronx (18.6%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H167vQeeJBMI"
      },
      "outputs": [],
      "source": [
        "rules = association_rules(frequent_itemsets,\n",
        "                          num_itemsets=frequent_itemsets.shape[0],\n",
        "                          metric=\"support\", min_threshold=0.001) #, metric=\"confidence\", min_threshold=0.6\n",
        "rules.sort_values(by=[\"support\", \"confidence\"])\n",
        "\n",
        "\n",
        "\n",
        "rules_filtered = rules[(rules['confidence'] > 0.2) & (rules['lift'] >= 1)]\n",
        "rules_filtered.sort_values(by=\"confidence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f80pMbglJEqZ"
      },
      "source": [
        "**Association Rules**\n",
        "\n",
        "* Strongest Associations – \"Street Light Condition\" → \"Brooklyn\" and \"Traffic Signal Condition\" → \"Brooklyn\" show the highest confidence and lift.\n",
        "\n",
        "* Implication – Complaints about street lights and traffic signals are significantly more likely to originate from Brooklyn.\n",
        "\n",
        "**Insights**\n",
        "\n",
        "* Geographic Trends – Noise-related complaints, such as \"Noise - Street/Sidewalk,\" are strongly associated with both Brooklyn and New York, while heating issues are more common in the Bronx.\n",
        "\n",
        "* Brooklyn's Complaint Patterns – Brooklyn shows strong relationships with various complaint types, including heating, illegal parking, and traffic signal conditions. High-confidence ( >50%) and high-lift (≥1) rules confirm that certain complaints are region-specific.\n",
        "\n",
        "* Actionable Takeaways – These insights can help city officials prioritize location-based concerns, particularly in managing noise and infrastructure complaints in Brooklyn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IWQGUdEbJH5f"
      },
      "outputs": [],
      "source": [
        "categorical_cols = [\"borough\", \"complaint_type\"]\n",
        "transactions = df_association_rules[categorical_cols].astype(str).values.tolist()\n",
        "\n",
        "te = TransactionEncoder()\n",
        "encoded_array = te.fit(transactions).transform(transactions)\n",
        "\n",
        "df_encoded = pd.DataFrame(encoded_array, columns=te.columns_)\n",
        "df_encoded\n",
        "\n",
        "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
        "frequent_itemsets.sort_values(by=\"support\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QUsIJnIJT5M"
      },
      "source": [
        "**Frequent Complaint Types**\n",
        "\n",
        "* Noise - Residential: High support of 11.21%, emphasizing the prevalence of noise complaints.\n",
        "\n",
        "* Parking-related Challenges: Blocked Driveway with support of 5.14% and Illegal Parking with support of 6.18% reflect ongoing parking-related issues.\n",
        "\n",
        "**Geographic Distribution of Complaints**\n",
        "\n",
        "* Bronx: Support of 18.70% and Manhattan with Support of 19.51% indicate a significant number of complaints.\n",
        "\n",
        "* Queens: Support of 22.73% and Brooklyn with the Highest support at 29.80% demonstrate the boroughs with the most reported issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S7QJ1aplKWy8"
      },
      "outputs": [],
      "source": [
        "rules = association_rules(frequent_itemsets,\n",
        "                          num_itemsets=frequent_itemsets.shape[0],\n",
        "                          metric=\"support\", min_threshold=0.01) #, metric=\"confidence\", min_threshold=0.6\n",
        "rules.sort_values(by=[\"support\", \"confidence\"])\n",
        "\n",
        "\n",
        "\n",
        "rules_filtered = rules[(rules['confidence'] > 0.4) & (rules['lift'] >= 1)]\n",
        "rules_filtered.sort_values(by=\"confidence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPf5gCFSKukm"
      },
      "source": [
        "**Association Rules**\n",
        "\n",
        "* Blocked Driveway Complaints – Significantly more common in Queens, with 40.89% confidence and a lift of 1.79.\n",
        "\n",
        "* Noise Complaints – Strongly linked to Manhattan, with 46.57% confidence and a lift of 2.39, meaning nearly half of all noise complaints originate from this borough.\n",
        "\n",
        "**Insights**\n",
        "\n",
        "* Borough-Specific Challenges – The data highlights unique issues faced by different boroughs, which can be used by city officials for targeted interventions.\n",
        "\n",
        "* Potential Solutions – Stricter noise regulations in Manhattan, improved road maintenance in Brooklyn and Queens, and enhanced traffic enforcement in Queens could help mitigate these concerns effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V_bGCKDvUG4s"
      },
      "outputs": [],
      "source": [
        "rules_filtered['antecedents'] = rules_filtered['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_filtered['consequents'] = rules_filtered['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "support_table = rules_filtered.pivot(index='antecedents', columns='consequents',\n",
        "values='support')\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(18, 12))  # Adjust the width and height\n",
        "sns.heatmap(support_table, annot=True, linewidths=0.5, linecolor='gray', cmap='coolwarm')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXbuj9jPUPvr"
      },
      "source": [
        "**As we can see from the visualisation above:**\n",
        "\n",
        "* Noise - Residential complaints are most frequent in the Bronx (highest association: 0.031).\n",
        "* Heat/Hot Water issues are prominent in Brooklyn and Manhattan, indicating heating concerns in these areas.\n",
        "* Blocked Driveway complaints are notable in Brooklyn, suggesting higher parking congestion.\n",
        "* Illegal Parking is more reported in Brooklyn compared to other boroughs.\n",
        "* Street Light Condition complaints are scattered, possibly due to data misclassification.\n",
        "* Noise-related issues (Residential & Street/Sidewalk) have stronger associations in the Bronx and New York (Manhattan), pointing to potential urban noise challenges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5kPqZEUmKwdu"
      },
      "outputs": [],
      "source": [
        "categorical_cols = [\"complaint_type\",\"agency\"]\n",
        "transactions = df_association_rules[categorical_cols].astype(str).values.tolist()\n",
        "\n",
        "te = TransactionEncoder()\n",
        "encoded_array = te.fit(transactions).transform(transactions)\n",
        "\n",
        "df_encoded = pd.DataFrame(encoded_array, columns=te.columns_)\n",
        "df_encoded\n",
        "\n",
        "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
        "frequent_itemsets.sort_values(by=\"support\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsFK8FVwK44e"
      },
      "source": [
        "**Frequent Itemsets**\n",
        "\n",
        "1. Noise-Residential  – The most frequent complaint type, with 11.2% support.\n",
        "\n",
        "2. NYPD – Handles the largest share of complaints, accounting for 34.2% support.\n",
        "\n",
        "3. HPD – Responsible for a significant portion of complaints, with 23.2% support.\n",
        "\n",
        "4. Rodent Complaints – Often involve the Department of Health and Mental Hygiene (DOHMH)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r-tZzdYIK6l1"
      },
      "outputs": [],
      "source": [
        "rules = association_rules(frequent_itemsets,\n",
        "                          num_itemsets=frequent_itemsets.shape[0],\n",
        "                          metric=\"support\", min_threshold=0.01) #, metric=\"confidence\", min_threshold=0.6\n",
        "rules.sort_values(by=[\"support\", \"confidence\"])\n",
        "\n",
        "\n",
        "\n",
        "rules_filtered = rules[(rules['confidence'] > 0.4) & (rules['lift'] >= 1)]\n",
        "rules_filtered.sort_values(by=\"confidence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtpbkV8jLK92"
      },
      "source": [
        "**Association Rules**\n",
        "\n",
        "* NYPD – Strongly associated with noise and traffic-related issues such as Noise - Vehicle, Illegal Parking and Blocked Driveway.\n",
        "\n",
        "* DSNY – Exclusively manages sanitation-related complaints like Sanitation Condition and Missed Collection (All Materials).\n",
        "\n",
        "* DOB – Linked to construction-related complaints, particularly General Construction/Plumbing.\n",
        "\n",
        "* HPD – Handles housing-related complaints, including Heating, Heat/Hot Water, and Paint/Plaster.\n",
        "\n",
        "**Insights**\n",
        "\n",
        "Complaints are highly agency-specific, with the NYPD, HPD, DSNY, and DOB each handling distinct issue categories. High lift values confirm that certain complaints are strongly tied to specific agencies.\n",
        "\n",
        "These insights can streamline resource allocation and enhance response efficiency. For example, if noise complaints are prevalent, the NYPD may require additional officers trained in noise enforcement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PDxOEOPQUy35"
      },
      "outputs": [],
      "source": [
        "rules_filtered['antecedents'] = rules_filtered['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_filtered['consequents'] = rules_filtered['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "support_table = rules_filtered.pivot(index='antecedents', columns='consequents',\n",
        "values='support')\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(18, 12))  # Adjust the width and height\n",
        "sns.heatmap(support_table, annot=True, linewidths=0.5, linecolor='gray', cmap='coolwarm')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hR8R-w5VHQ7"
      },
      "source": [
        "**As we can see from the visualisation above:**\n",
        "\n",
        "* NYPD handles most Noise - Residential complaints (0.11).\n",
        "* HPD sees frequent HEAT/HOT WATER issues (0.068).\n",
        "* DSNY struggles with missed waste collection (0.062).\n",
        "* DOT faces infrastructure concerns (Street: 0.043, Lights: 0.031).\n",
        "* HPD & DEP handle building and sewer issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qlLflKjMLb3B"
      },
      "outputs": [],
      "source": [
        "df_association_rules['created_date'] = pd.to_datetime(df_association_rules['created_date'])\n",
        "\n",
        "df_association_rules['day_of_week'] = df_association_rules['created_date'].dt.day_name()\n",
        "\n",
        "categorical_cols = [\"complaint_type\",\"day_of_week\"]\n",
        "transactions = df_association_rules[categorical_cols].astype(str).values.tolist()\n",
        "\n",
        "te = TransactionEncoder()\n",
        "encoded_array = te.fit(transactions).transform(transactions)\n",
        "\n",
        "df_encoded = pd.DataFrame(encoded_array, columns=te.columns_)\n",
        "df_encoded\n",
        "\n",
        "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
        "frequent_itemsets.sort_values(by=\"support\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYQW7hXILte3"
      },
      "source": [
        "**Frequent Itemsets**\n",
        "\n",
        "There is a high support for the combinations of **\"Noise - Residential\"** complaints with both Saturdays and Sundays, indicating they are the most frequently reported complaint type during the weekends.\n",
        "\n",
        "Other complaints like **\"Heat/Hot Water,\" \"Blocked Driveway,\" and \"Illegal Parking\"** also have relatively high support values, highlighting persistent issues across different days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6LabJeMSLuki"
      },
      "outputs": [],
      "source": [
        "rules = association_rules(frequent_itemsets,\n",
        "                          num_itemsets=frequent_itemsets.shape[0],\n",
        "                          metric=\"support\", min_threshold=0.01) #, metric=\"confidence\", min_threshold=0.6\n",
        "rules.sort_values(by=[\"support\", \"confidence\"])\n",
        "\n",
        "\n",
        "\n",
        "rules_filtered = rules[(rules['confidence'] > 0.2) & (rules['lift'] >= 1)]\n",
        "rules_filtered.sort_values(by=\"confidence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0oXf3i8LzuU"
      },
      "source": [
        "**Association Rules**\n",
        "\n",
        "* Noise Complaints on Saturday: There is a 21.1% chance of a complaint being about noise on Saturday.\n",
        "\n",
        "* Lift Value: A lift of 1.88 indicates a significant increase in noise complaints on Saturdays compared to expected frequency.\n",
        "\n",
        "* Weekend Relationship: If a noise complaint is reported, it is likely to occur on Saturdays and Sundays.\n",
        "\n",
        "**Insights**\n",
        "\n",
        "* Resource Allocation: City officials should allocate more resources to address noise complaints during weekends, especially on Saturdays and Sundays.\n",
        "\n",
        "* Enforcement Personnel: Increasing the availability of enforcement personnel on these days could enhance response efforts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ie92kgrkVb7i"
      },
      "outputs": [],
      "source": [
        "rules_filtered['antecedents'] = rules_filtered['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_filtered['consequents'] = rules_filtered['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "support_table = rules_filtered.pivot(index='antecedents', columns='consequents',\n",
        "values='support')\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(18, 12))  # Adjust the width and height\n",
        "sns.heatmap(support_table, annot=True, linewidths=0.5, linecolor='gray', cmap='coolwarm')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RPbugc9Vwbb"
      },
      "source": [
        "**As we can see from the visualisation above:**\n",
        "\n",
        "The association values (0.027) are consistent across the matrix, suggesting a uniform pattern but Residential noise tends to be reported more on weekends, as seen in the relationship between \"Noise - Residential\" and Saturday/Sunday.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ahzKy9rtL6zF"
      },
      "outputs": [],
      "source": [
        "categorical_cols = [\"complaint_type\",\"open_data_channel_type\"]\n",
        "transactions = df_association_rules[categorical_cols].astype(str).values.tolist()\n",
        "\n",
        "te = TransactionEncoder()\n",
        "encoded_array = te.fit(transactions).transform(transactions)\n",
        "\n",
        "df_encoded = pd.DataFrame(encoded_array, columns=te.columns_)\n",
        "df_encoded\n",
        "\n",
        "frequent_itemsets = apriori(df_encoded, min_support=0.05, use_colnames=True)\n",
        "frequent_itemsets.sort_values(by=\"support\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8cQWf0sMBPa"
      },
      "source": [
        "**Frequent Itemsets**\n",
        "\n",
        "* Phone Channel: The \"Phone\" channel is the most frequently used method for reporting complaints, indicating residents' preference for calling over other channels.\n",
        "\n",
        "* Online Channel: The \"Online\" channel is the second most common reporting method, showing significant digital engagement among residents.\n",
        "\n",
        "* Noise Complaints: Noise - Residential complaints are prevalent, highlighting the need for urban management to address these issues.\n",
        "\n",
        "* Other Common Complaints: Complaints like \"Blocked Driveway\" (support = 0.051418), \"Illegal Parking\" (support = 0.061814), and \"Heat/Hot Water\" (support = 0.067758) are also frequently faced by residents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "17ra07LtMCPP"
      },
      "outputs": [],
      "source": [
        "rules = association_rules(frequent_itemsets,\n",
        "                          num_itemsets=frequent_itemsets.shape[0],\n",
        "                          metric=\"support\", min_threshold=0.01) #, metric=\"confidence\", min_threshold=0.6\n",
        "rules.sort_values(by=[\"support\", \"confidence\"])\n",
        "\n",
        "rules_filtered = rules[(rules['confidence'] > 0.1) & (rules['lift'] >= 0.1)]\n",
        "rules_filtered.sort_values(by=\"confidence\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHhty_EjMFoK"
      },
      "source": [
        "**Association Rules**\n",
        "\n",
        "The significant confidence (44.7%) indicates that noise complaints are predominantly reported via the phone. This suggests that residents may feel more compelled to express urgency or frustration over noise disturbances directly.\n",
        "\n",
        "**Insights**\n",
        "\n",
        "* Improving Responsiveness: Enhancing the responsiveness of phone reporting systems could improve resident satisfaction, given the frequency of noise complaints reported via phone.\n",
        "\n",
        "* Exploring Additional Mediums:  It would be beneficial to explore additional mediums, such as in-person or online reporting, to alleviate some pressure from phone calls for registering complaints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CCpvOFYMWaz9"
      },
      "outputs": [],
      "source": [
        "rules_filtered['antecedents'] = rules_filtered['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_filtered['consequents'] = rules_filtered['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "support_table = rules_filtered.pivot(index='antecedents', columns='consequents',\n",
        "values='support')\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(18, 12))  # Adjust the width and height\n",
        "sns.heatmap(support_table, annot=True, linewidths=0.5, linecolor='gray', cmap='coolwarm')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b7sn2xa7DQFw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# Ensuring correct data types\n",
        "df_association_rules['complaint_type'] = df_association_rules['complaint_type'].astype(str)\n",
        "df_association_rules['borough'] = df_association_rules['borough'].astype(str)\n",
        "\n",
        "# Convert resolution_time into quartile-based bins\n",
        "df_association_rules['resolution_time'] = pd.qcut(df_association_rules['resolution_time'].astype(float), q=4, labels=['Very Fast', 'Fast', 'Slow', 'Very Slow'])\n",
        "df_association_rules['resolution_time'] = df_association_rules['resolution_time'].astype(str)\n",
        "\n",
        "# Creating transactions\n",
        "transactions1 = df_association_rules[['borough', 'complaint_type', 'resolution_time']].apply(tuple, axis=1).tolist()\n",
        "transactions1 = [[t] if isinstance(t, str) else list(t) for t in transactions1]\n",
        "\n",
        "# Applying Transaction Encoder\n",
        "te1 = TransactionEncoder()\n",
        "trans_array1 = te1.fit_transform(transactions1)\n",
        "transaction1_df = pd.DataFrame(trans_array1, columns=te1.columns_)\n",
        "\n",
        "# Applying Apriori Algorithm\n",
        "complaint_patterns1 = apriori(transaction1_df, min_support=0.01, use_colnames=True)\n",
        "complaint_patterns1 = complaint_patterns1.sort_values(by='support', ascending=False)\n",
        "\n",
        "# Generating Association Rules\n",
        "complaint_rules1 = association_rules(complaint_patterns1, metric='confidence', min_threshold=0.3)\n",
        "\n",
        "# Filtering rules where resolution_time appears as a consequent\n",
        "complaint_rules1 = complaint_rules1[\n",
        "    (complaint_rules1['antecedents'].astype(str).str.count(\",\") >= 1) &\n",
        "    (complaint_rules1['consequents'].astype(str).str.contains(\"Very Fast|Fast|Slow|Very Slow\"))\n",
        "]\n",
        "\n",
        "# Sorting by support and confidence\n",
        "complaint_rules1 = complaint_rules1.sort_values(by=['support', 'confidence'], ascending=False)\n",
        "complaint_rules1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w1r_EUSVniA"
      },
      "source": [
        "**Noise**\n",
        "\n",
        "- Noise complaints across boroughs show different resolution speeds  \n",
        "- In Bronx residential noise complaints have a 54 percent confidence of being resolved quickly Fast  \n",
        "- In Queens the confidence level for a Fast resolution is 51 percent  \n",
        "- In Brooklyn and Manhattan noise complaints are more likely to be handled Very Fast with confidence levels of 47 percent and 53 percent respectively  \n",
        "- Manhattan and Brooklyn have the most efficient noise complaint resolution systems  \n",
        "- Bronx and Queens still resolve noise complaints quickly but with slightly lower efficiency\n",
        "\n",
        "Lift values between 1.5 - 2.1 indicate a strong association, meaning that noise complaints in these boroughs frequently result in quicker resolutions compared to other complaints.\n",
        "Manhattan has the highest resolution efficiency for noise-related complaints, followed by Brooklyn.\n",
        "\n",
        "**Heat**\n",
        "\n",
        "- Heat and hot water complaints in the Bronx and Brooklyn take longer to resolve especially during colder months  \n",
        "- In the Bronx there is a 65 percent confidence that these complaints will be handled slowly while in Brooklyn it is 63 percent  \n",
        "- The strong correlation suggests that heating issues are more likely to face delays compared to other types of complaints  \n",
        "- High demand during winter puts extra pressure on response teams and slows down resolution times in these areas\n",
        "\n",
        "**Parking**\n",
        "\n",
        "- Illegal parking and blocked driveway complaints in Brooklyn and Queens are generally resolved quickly  \n",
        "- In Brooklyn illegal parking complaints have a 55 percent confidence of being addressed at a fast pace while in Queens it is 47 percent  \n",
        "- Blocked driveway complaints in Brooklyn are handled even more efficiently with a 59 percent confidence of fast resolution  \n",
        "- The lift values between 1.8 and 2.6 indicate that these types of complaints are resolved faster than average  \n",
        "- Parking related issues are a priority and are being handled efficiently in these boroughs\n",
        "\n",
        "**Recommendations**\n",
        "\n",
        "- Allocate more resources to the Bronx and Brooklyn to improve heat complaint response times in winter.  \n",
        "- Implement a proactive maintenance program during colder months to reduce delays.  \n",
        "- Expand noise monitoring and reporting tools in the Bronx and Queens to improve efficiency.  \n",
        "- Maintain strong traffic enforcement and towing operations in Brooklyn and Queens for parking and blocked driveway complaints.  \n",
        "- Optimize staffing by increasing specialists for heating complaints in the Bronx and Brooklyn.  \n",
        "- Expand noise enforcement beyond Manhattan and Brooklyn to other boroughs.  \n",
        "- Continue strong traffic enforcement in Queens and Brooklyn to manage illegal parking effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_E4k1zrZC0dS"
      },
      "outputs": [],
      "source": [
        "# Ensuring correct data types\n",
        "df_association_rules['complaint_type'] = df_association_rules['complaint_type'].astype(str)\n",
        "df_association_rules['borough'] = df_association_rules['borough'].astype(str)\n",
        "df_association_rules['agency'] = df_association_rules['agency'].fillna(\"Unknown\").astype(str)\n",
        "\n",
        "# Creating transactions where Each row is (Borough + Complaint Type) → Agency Handling Request\n",
        "transactions2 = df_association_rules[['borough', 'complaint_type', 'agency']].apply(tuple, axis=1).tolist()\n",
        "transactions2 = [[t] if isinstance(t, str) else list(t) for t in transactions2]\n",
        "\n",
        "# Apply Transaction Encoder\n",
        "te2 = TransactionEncoder()\n",
        "trans_array2 = te2.fit_transform(transactions2)\n",
        "transaction2_df = pd.DataFrame(trans_array2, columns=te2.columns_)\n",
        "\n",
        "# Lower support threshold to detect more frequent itemsets\n",
        "complaint_patterns2 = apriori(transaction2_df, min_support=0.01, use_colnames=True)\n",
        "complaint_patterns2 = complaint_patterns2.sort_values(by='support', ascending=False)\n",
        "\n",
        "complaint_rules2 = association_rules(complaint_patterns2, metric='confidence', min_threshold=0.3)\n",
        "\n",
        "# Extract list of agency names from the dataset\n",
        "agency_list = [\n",
        "    'HPD', 'DSNY', 'DOF', 'DOT', 'NYPD', 'DHS', 'DEP', 'DOB', 'DPR', 'DFTA', '3-1-1',\n",
        "    'DOE', 'DCA', 'DOHMH', 'TLC', 'DOITT', 'EDC', 'NYCEM', 'HRA', 'DORIS',\n",
        "    'MAYORS OFFICE OF SPECIAL ENFORCEMENT'\n",
        "]\n",
        "\n",
        "# Keeping rules where the antecedents have multiple items and the consequents contain only agency names\n",
        "complaint_rules2 = complaint_rules2[\n",
        "    (complaint_rules2['antecedents'].astype(str).str.count(\",\") >= 1) &\n",
        "    (complaint_rules2['consequents'].astype(str).apply(lambda x: any(agency in str(x) for agency in agency_list))) &\n",
        "    (complaint_rules2['lift'] > 1)\n",
        "]\n",
        "\n",
        "# Sort and display results\n",
        "complaint_rules2 = complaint_rules2.sort_values(by=['support', 'confidence'], ascending=False)\n",
        "complaint_rules2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyIOB9zeZ09m"
      },
      "source": [
        "### Noise Complaints and Agency Handling  \n",
        "- Noise complaints across boroughs are predominantly handled by NYPD.  \n",
        "- In Brooklyn Bronx Manhattan and Queens residential noise complaints have a 100 percent confidence of being assigned to NYPD.  \n",
        "- The lift value of 2.92 suggests that noise complaints are nearly three times more likely to be assigned to NYPD compared to other agencies.  \n",
        "\n",
        "### Illegal Parking and Blocked Driveway Complaints  \n",
        "- Illegal parking complaints in Brooklyn and Queens are also 100 percent assigned to NYPD.  \n",
        "- Blocked driveway complaints in Brooklyn and Queens also show 100 percent confidence of being handled by NYPD.  \n",
        "- Lift values of 2.92 indicate a strong enforcement process ensuring quick response and resolution for parking-related complaints.  \n",
        "\n",
        "### Heat and Hot Water Complaints  \n",
        "- HPD Housing Preservation and Development is exclusively responsible for heat and hot water complaints.  \n",
        "- In the Bronx and Brooklyn there is a 100 percent confidence level that heat-related complaints will be managed by HPD.  \n",
        "- The lift value of 4.30 means that these complaints are more than four times as likely to be assigned to HPD than any other agency.  \n",
        "\n",
        "### Street Condition Complaints  \n",
        "- DOT Department of Transportation handles street condition complaints.  \n",
        "- Complaints related to street conditions in Brooklyn and Queens have a 100 percent confidence of being assigned to DOT.  \n",
        "- The lift value of 8.14 suggests that street-related complaints are highly correlated with DOT involvement.  \n",
        "\n",
        "### Noise Complaints and DEP  \n",
        "- In Manhattan some noise complaints are also assigned to DEP Department of Environmental Protection.  \n",
        "- The lift value of 11.96 suggests that DEP is highly specialized in handling specific noise complaints possibly related to environmental factors.  \n",
        "\n",
        "### Recommendation\n",
        "To improve response efficiency NYPD should allocate more resources for noise and parking enforcement in high-complaint areas like Brooklyn and Queens. HPD should expand its capacity for heat complaints in winter particularly in the Bronx and Brooklyn where delays are more likely. DOT can enhance street maintenance programs in Brooklyn and Queens to prevent recurring issues. Better coordination between DEP and NYPD in Manhattan could improve response times for noise complaints given DEP's strong role in environmental noise issues. Finally increasing public awareness about which agencies handle specific complaints can help residents report issues correctly reducing delays and improving overall efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lpq1Z3PxBwoB"
      },
      "outputs": [],
      "source": [
        "# Ensure correct data types\n",
        "df_association_rules['complaint_type'] = df_association_rules['complaint_type'].astype(str)\n",
        "df_association_rules['borough'] = df_association_rules['borough'].astype(str)\n",
        "df_association_rules['status'] = df_association_rules['status'].astype(str)  # Ensure status is categorical\n",
        "\n",
        "# Create transactions where Each row is (Complaint Type + Borough) → Status\n",
        "transactions3 = df_association_rules[['complaint_type', 'borough', 'status']].apply(tuple, axis=1).tolist()\n",
        "transactions3 = [[t] if isinstance(t, str) else list(t) for t in transactions3]\n",
        "\n",
        "# Checking transactions\n",
        "print(\"Sample transactions:\", transactions3[:5])\n",
        "\n",
        "# Applying Transaction Encoder\n",
        "te3 = TransactionEncoder()\n",
        "trans_array3 = te3.fit_transform(transactions3)\n",
        "transaction3_df = pd.DataFrame(trans_array3, columns=te3.columns_)\n",
        "\n",
        "# Checking encoded columns\n",
        "print(\"Transaction DataFrame Columns:\", transaction3_df.columns.tolist())\n",
        "\n",
        "# Applying Apriori Algorithm\n",
        "complaint_patterns3 = apriori(transaction3_df, min_support=0.01, use_colnames=True)\n",
        "complaint_patterns3 = complaint_patterns3.sort_values(by='support', ascending=False)\n",
        "\n",
        "# Generating Association Rules\n",
        "complaint_rules3 = association_rules(complaint_patterns3, metric='confidence', min_threshold=0.3)\n",
        "\n",
        "# Filtering rules where status appears as a consequent\n",
        "complaint_rules3 = complaint_rules3[\n",
        "    (complaint_rules3['antecedents'].astype(str).str.count(\",\") >= 1) &\n",
        "    (complaint_rules3['consequents'].astype(str).str.contains(\"Closed|Open|Pending|Resolved\"))\n",
        "]\n",
        "\n",
        "# Sorting by support and confidence\n",
        "complaint_rules3 = complaint_rules3.sort_values(by=['support', 'confidence'], ascending=False)\n",
        "complaint_rules3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8_177X5CIpI"
      },
      "source": [
        "This association rule analysis did not yield strong results due to the following reasons:  \n",
        "\n",
        "1. Lift values close to 1  \n",
        "   - Lift measures how much more likely the consequent for example closed is to occur given the antecedents for example borough or complaint type compared to random chance.  \n",
        "   - A lift value close to 1 suggests that the antecedent does not significantly increase the likelihood of the consequent occurring meaning the discovered relationships lack strong predictive power.  \n",
        "\n",
        "2. High confidence but weak predictive power  \n",
        "   - Many rules show high confidence values close to 1.0 but because lift is around 1 it means that the antecedent does not provide useful insight beyond what we would already expect from overall complaint closure rates.  \n",
        "   - Essentially closures happen frequently regardless of the antecedent so the rule is not revealing anything new.  \n",
        "\n",
        "3. Higher lift but low support  \n",
        "   - Some rules with higher lift for example 2.38 for noise to Manhattan have very low support meaning they occur infrequently in the dataset.  \n",
        "   - Low support means these patterns are not common enough to be actionable making them statistically weak and unreliable for decision-making.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShTJKNNBuCor"
      },
      "source": [
        "**Summary of Association Rules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXh7_K4qQawq"
      },
      "source": [
        "Association Rule Mining on the NYC 311 dataset uncovered key complaint patterns across boroughs, agencies, and time.\n",
        "\n",
        "* Noise complaints dominate Manhattan and Brooklyn, while illegal parking and blocked driveways are frequent in Queens and Brooklyn.\n",
        "\n",
        "* Heat and hot water issues in the Bronx and Brooklyn face delays, especially in winter, highlighting the need for better resource planning.\n",
        "\n",
        "* NYPD manages noise and parking, HPD handles heating, and DOT oversees street conditions.\n",
        "\n",
        "Strengthening inter-agency coordination and raising public awareness of agency roles can further streamline complaint resolution.\n",
        "\n",
        "To improve response times, the city should increase weekend enforcement for noise, expand winter heating teams, and enhance traffic management in high-complaint areas. These insights can help optimize city services and improve urban living conditions for NYC residents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF7sfzFCKpku"
      },
      "source": [
        "## **2. Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz0O0YhIy9oB"
      },
      "source": [
        "### **2.1 Heirarchical Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSO0HBrWy_GT"
      },
      "source": [
        "**Goal**\n",
        "\n",
        "To explore clustering patterns in NYC 311 service requests, we applied hierarchical clustering using three key numerical features:\n",
        "\n",
        "* latitude and longitude – To group complaints based on geographic proximity and identify spatial clusters of service requests.\n",
        "* zip_complaint_density – To capture the frequency of complaints within different ZIP codes and understand which areas experience the highest complaint volumes.\n",
        "\n",
        "By clustering complaints using these features, our goal was to identify geographic hotspots where high volumes of service requests occur, enabling city officials to allocate resources more effectively. Understanding complaint density patterns can help streamline response strategies, reduce resolution times, and improve urban management.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "Hierarchical clustering, while useful for smaller datasets, is computationally infeasible for our dataset due to several limitations.\n",
        "\n",
        "- High Memory & Computation – Requires O(n²) memory, leading to 7.5 billion distance calculations, causing system crashes.\n",
        "- Poor Scalability – Suitable for ≤5,000 rows, making it inefficient for large datasets.\n",
        "- Uninterpretable Dendrogram – With 86,877 points, the dendrogram is cluttered and unusable.\n",
        "- Pairwise Distance Overhead – Computing all pairwise distances is computationally prohibitive.\n",
        "\n",
        "**Approach Used**\n",
        "\n",
        "To mitigate these issues, we sampled 5,000 rows and performed hierarchical clustering using Ward’s method, which minimizes variance within clusters. We assigned 5 clusters (for 5 boroughs in New York) by cutting the dendrogram at an appropriate threshold. While this allowed for some spatial insights, the approach remains unsuitable for the full dataset due to excessive computational demands.\n",
        "\n",
        "**Next Step : Calculate Silhouette Score**\n",
        "\n",
        " We will calculate the **silhouette score** to identify the optimal number of clusters for accuracy and segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kzsfVcCay-md"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Sampling\n",
        "sample_size = 5000\n",
        "df_numerical = df[numerical_columns]\n",
        "selected_features = ['latitude', 'longitude', 'zip_complaint_density']\n",
        "df_sample = df_numerical[selected_features].sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "df_sample_normalized = (df_sample - df_sample.min()) / (df_sample.max() - df_sample.min())\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "Z = linkage(df_sample_normalized, method='ward')\n",
        "\n",
        "silhouette_scores = []\n",
        "\n",
        "for num_cluster in range(2, 10):  # Testing clusters from 2 to 9\n",
        "    labels = fcluster(Z, num_cluster, criterion='maxclust')\n",
        "    score = silhouette_score(df_sample_normalized, labels)\n",
        "    silhouette_scores.append((num_cluster, score))\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_scores = pd.DataFrame(silhouette_scores, columns=['num_clusters', 'silhouette_score'])\n",
        "\n",
        "# Plot the silhouette scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(df_scores['num_clusters'], df_scores['silhouette_score'], marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score for Different Cluster Sizes')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2rfGqU0zlRy"
      },
      "source": [
        "**Interpretation : Silhouette Score**\n",
        "\n",
        "- The silhouette score increases with more clusters, peaking at k=6 and k=9, indicating these values provide the most distinct groupings.\n",
        "- k=6 appears optimal, balancing cluster quality and interpretability, making it suitable for segmenting NYC 311 complaints effectively.\n",
        "\n",
        "\n",
        "**Next Step: Calculate Distortion Score**\n",
        "\n",
        "We will calculate the distortion score to measure cluster compactness and determine the optimal number of clusters for better segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nMvXlwlozmLk"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Define range of clusters\n",
        "cluster_range = range(2, 7)\n",
        "distortions = []\n",
        "\n",
        "for k in cluster_range:\n",
        "    # Agglomerative Clustering\n",
        "    model = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
        "    labels = model.fit_predict(df_sample[selected_features])\n",
        "\n",
        "    # Cluster centroids\n",
        "    centroids = df_sample[selected_features].groupby(labels).mean().values\n",
        "\n",
        "    # Distortion score\n",
        "    distortion = np.sum(np.min(cdist(df_sample[selected_features], centroids, 'euclidean'), axis=1) ** 2)\n",
        "\n",
        "    distortions.append(distortion)\n",
        "\n",
        "# Plotting Elbow Curve\n",
        "plt.plot(cluster_range, distortions, marker='o', linestyle='-')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Distortion Score\")\n",
        "plt.title(\"Elbow Method for Hierarchical Clustering\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnrQoRT0zsPV"
      },
      "source": [
        "**Ideal Number of Clusters: k=5**\n",
        "1. Balancing Compactness and Interpretability\n",
        "  - While the silhouette score peaks at k=6, k=5 offers comparable clarity while avoiding unnecessary segmentation.\n",
        "  - Additional clusters (k=6 or k=9) provide finer distinctions, but the improvement is minimal.\n",
        "2. Distortion Score and Diminishing Returns\n",
        "  - The Elbow Method shows a sharp drop in distortion until k=5, after which improvements slow down.\n",
        "  - k=5 strikes a balance between compact clusters and interpretability.\n",
        "3. Practical Considerations for NYC 311 Complaints\n",
        "  - More clusters (e.g., k=6 or k=9) may overcomplicate city resource planning.\n",
        "  - k=5 effectively segments NYC neighborhoods based on complaint patterns, ensuring actionable insights for city management.\n",
        "\n",
        "**Next Step: Implementing Clustering**\n",
        "\n",
        "Based on our analysis of both silhouette scores and the Elbow Method, we will proceed with hierarchical clustering using k=5 clusters. The goal is to achieve a balance between segmentation quality and interpretability, ensuring that the results are actionable for NYC 311 service management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OIezsgZNzpMR"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "\n",
        "selected_features = ['latitude', 'longitude', 'zip_complaint_density']\n",
        "\n",
        "# Hierarchical Clustering with 5 clusters\n",
        "Z = linkage(df_sample, method='ward')\n",
        "\n",
        "n_clusters = 5\n",
        "df_sample['cluster_labels'] = fcluster(Z, n_clusters, criterion='maxclust')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tf5dCQ_QzwWY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "plt.style.use('default')\n",
        "fig, ax = plt.subplots(figsize=(10, 5), facecolor='white')\n",
        "threshold = Z[-n_clusters, 2]\n",
        "\n",
        "# Plot the dendrogram\n",
        "dendrogram(Z, color_threshold=threshold, above_threshold_color='gray', ax=ax)\n",
        "\n",
        "# Customize plot\n",
        "plt.title(\"Dendrogram for Hierarchical Clustering\")\n",
        "plt.xlabel(\"Data Points\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LUymoMOz0bQ"
      },
      "source": [
        "**Interpretation : Dendrogram**\n",
        "\n",
        "- The early merges indicate closely related data points, while larger branches represent distinct clusters.\n",
        "\n",
        "- The five main clusters suggest groupings based on geography and complaint density. However, the dense lower section highlights hierarchical clustering’s limitation for large datasets, making interpretation difficult."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_LlPspKAz1Rq"
      },
      "outputs": [],
      "source": [
        "# Scatter Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df_sample[\"longitude\"], y=df_sample[\"latitude\"], hue=df_sample[\"cluster_labels\"], palette=\"Set2\", s=18)\n",
        "plt.title(\"Geospatial Hierarchical Clustering (5 clusters)\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaJPhegJz925"
      },
      "source": [
        "**Interpretation : Scatter Plot**\n",
        "\n",
        "Clusters align with NYC boroughs, capturing distinct geographic complaint patterns.\n",
        "\n",
        "- High-density clusters (Cluster 2 - Bronx, Cluster 4 - Brooklyn, Cluster 3 - Manhattan) suggest concentrated urban issues, likely influenced by population density, urban infrastructure, and service request volume.\n",
        "\n",
        "- More dispersed clusters (Cluster 1 - Staten Island, Cluster 5 - Queens) indicate lower complaint density, which aligns with these areas being more residential and less densely populated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oStalEPbz-aE"
      },
      "outputs": [],
      "source": [
        "# Pairplot\n",
        "sns.pairplot(df_sample, vars=selected_features, hue=\"cluster_labels\", palette=\"Set2\", diag_kind=\"hist\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_6RJBsB0Cgy"
      },
      "source": [
        "**Interpretation : Pairplot**\n",
        "\n",
        "- Histograms reveal spatial and complaint density variations, confirming that complaint patterns are not uniform across NYC, emphasizing the need for location-based city management.\n",
        "- High-density clusters (green, orange) vs. low-density (purple) suggest certain neighborhoods face frequent service issues, requiring targeted interventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Kbnkbp5V0DZk"
      },
      "outputs": [],
      "source": [
        "# 3D Scatter Plot\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_3d(df_sample,\n",
        "                    x='longitude',\n",
        "                    y='latitude',\n",
        "                    z='zip_complaint_density',\n",
        "                    color=df_sample['cluster_labels'].astype(str),\n",
        "                    opacity=0.5,\n",
        "                    color_discrete_sequence=px.colors.qualitative.Set2)\n",
        "\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "fig.update_layout(title=\"3D Scatter Plot of Hierarchical Clustering\",\n",
        "                  scene=dict(xaxis_title=\"Longitude\",\n",
        "                             yaxis_title=\"Latitude\",\n",
        "                             zaxis_title=\"Complaint Density\"))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIW-6r900Iig"
      },
      "source": [
        "**Interpretation : 3-D Scatter Plot**\n",
        "\n",
        "- The clustering reveals a spatial distribution of complaints across NYC, with each cluster representing a distinct geographic region or complaint density level.\n",
        "- Some clusters show higher complaint density, highlighting regions with increased service demand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0yGv5cT0LvN"
      },
      "source": [
        "**Summary of Hierarchical Clustering**\n",
        "\n",
        "**Findings**\n",
        "- Five distinct clusters were identified, aligning with NYC’s geographic regions and complaint density.\n",
        "- High-density clusters (e.g., Bronx, Manhattan, Brooklyn) suggest concentrated service demands, while low-density clusters (e.g., Staten Island, parts of Queens) indicate dispersed complaints.\n",
        "\n",
        "\n",
        "**Analysis**\n",
        "\n",
        "1. The clustering effectively captures spatial complaint distribution, highlighting areas with persistent service demands.\n",
        "2. Overlap between some clusters suggests that location alone is not always sufficient for differentiation; additional features may improve segmentation.\n",
        "3. The hierarchical structure reveals relationships between high- and low-density areas but becomes less interpretable as dataset size increases.\n",
        "\n",
        "\n",
        "**Impact on NYC 311 Service Optimization**\n",
        "\n",
        "This clustering helps NYC 311 service authorities prioritize resources, improve response times, and proactively address recurring issues.\n",
        "\n",
        "Identifying high-density complaint areas enables:\n",
        "\n",
        "- Targeted resource allocation for faster resolution.\n",
        "- Early detection of emerging service issues based on geographic complaint trends.\n",
        "- Data-driven policy-making to enhance city service efficiency.\n",
        "\n",
        "**Recommendations for NYC 311 Authorities**\n",
        "\n",
        "1. Increase staffing and resources in high-complaint areas to reduce response delays.\n",
        "2. Analyze complaint types within each cluster to tailor service responses.\n",
        "3. Implement proactive service interventions in recurring issue zones.\n",
        "4. Use temporal clustering to anticipate seasonal complaint surges and optimize response planning.\n",
        "5. Integrate additional data (e.g., complaint type, resolution time) to refine clustering and enhance service strategy.\n",
        "\n",
        "**Next Steps: Applying K-Means Clustering**\n",
        "\n",
        "- Implement K-Means clustering for a scalable, computationally efficient alternative.\n",
        "- Compare cluster quality and interpretability with hierarchical results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on_j1g0c0SQq"
      },
      "source": [
        "### **2.2 K-Means Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VHfSsSa0Yg0"
      },
      "source": [
        "**Goal**\n",
        "\n",
        "We applied K-means clustering to uncover patterns in NYC 311 service requests using key numerical features (e.g., latitude, longitude, zip_complaint_density, etc.).\n",
        "\n",
        "Our aim is to:\n",
        "\n",
        "- Identify Geographically Similar Areas – Cluster neighborhoods or regions that share similar latitude/longitude or complaint densities.\n",
        "- Optimize Resource Allocation – Reveal hotspots and high-density complaint clusters so city officials can better target staffing and resources.\n",
        "- Enhance Urban Management – Use the cluster assignments to tailor response strategies to each group’s needs.\n",
        "\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "While K-means scales better than hierarchical clustering for large datasets, it still has certain drawbacks:\n",
        "\n",
        "- Sensitivity to Initialization – K-means can converge to a local optimum, requiring multiple runs or better initialization schemes.\n",
        "- Cluster Shape Constraints – Tends to form spherical clusters, which may not always match real-world data distributions.\n",
        "\n",
        "**Approach Used**\n",
        "1. Feature Selection & Sampling\n",
        "  - Focused on key numeric columns (e.g., latitude, longitude, resolution times) and took a 5,000-row sample from the 86,877 records for efficient computation.\n",
        "2. Data Preprocessing\n",
        "  - PCA: Reduced dimensionality to retain ~90% variance, streamlining clustering performance and interpretation.\n",
        "3. Cluster Determination\n",
        "  - Ran K-means with different k values (2–10), evaluated using elbow method (distortion) and silhouette scores.\n",
        "  - Chose the optimal k based on highest silhouette or elbow “bend.”\n",
        "\n",
        "**Analysis & Visualization**\n",
        "  - Assigned cluster labels to each sample record, plotted them in reduced (PCA) space to check separation.\n",
        "  - Examined cluster centroids and key statistics to understand the complaint patterns in each cluster.\n",
        "\n",
        "\n",
        "This procedure delivers a scalable clustering approach, helping us group service requests into meaningful clusters for improved city management while acknowledging K-means’ limitations regarding outliers and cluster shape assumptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8S8otxVlFtF"
      },
      "source": [
        "Step 1: Create a new dataframe with numerical columns only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I2L4w5YjfkCb"
      },
      "outputs": [],
      "source": [
        "numerical_columns = [\n",
        "    'created_hour_of_day', 'day_of_week', 'resolution_time_minutes',\n",
        "    'latitude', 'longitude', 'zip_complaint_density', 'complaint_type_score',\n",
        "    'response_delay_hours', 'borough_complaint_density', 'weekend_vs_weekday_complaints'\n",
        "]\n",
        "df_numeric = df[numerical_columns].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiP7i3U41FN2"
      },
      "source": [
        "Step 2: Take a sample of 5000 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1nOa8so61DL7"
      },
      "outputs": [],
      "source": [
        "df_sample = df_numeric.sample(n=5000, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUgFCWAM1KKK"
      },
      "source": [
        "Step 3: Do an initial PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fyzCkQw11IBq"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_sample)\n",
        "\n",
        "pca_all = PCA(random_state=42)\n",
        "X_pca_all = pca_all.fit_transform(X_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411ORbMb1MRQ"
      },
      "source": [
        "Step 4: Check PCA Explained Variance (Scree Plot & Ratios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "falD7oHn1OeY"
      },
      "outputs": [],
      "source": [
        "explained_variance_ratio = pca_all.explained_variance_ratio_\n",
        "cumulative_variance = explained_variance_ratio.cumsum()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(explained_variance_ratio)+1), cumulative_variance, marker='o')\n",
        "plt.axhline(y=0.90, color='r', linestyle='--')\n",
        "plt.title('Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Variance')\n",
        "plt.show()\n",
        "\n",
        "for i, ratio in enumerate(cumulative_variance, start=1):\n",
        "    if ratio >= 0.90:\n",
        "        print(f\"PCs needed to retain at least 90% variance: {i}\")\n",
        "        break\n",
        "\n",
        "n_components_needed = i  # from the above loop\n",
        "print(f\"Using {n_components_needed} PCs to retain ~90% of variance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-fvxgjW1UUu"
      },
      "source": [
        "**Interpretation**\n",
        "\n",
        "- Around 7 principal components capture roughly 90% of the total variance in the data.\n",
        "- Beyond 7 PCs, each additional component contributes only minimal variance, so 7 is a good balance between dimensionality and information retention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuRI7fd91X8E"
      },
      "source": [
        "Step 5: Outlier Removal using Reconstruction Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lQLL8Nar1U3x"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "pca_outlier = PCA(n_components=3, random_state=42)\n",
        "X_pca_outlier = pca_outlier.fit_transform(X_scaled)\n",
        "X_reconstructed = pca_outlier.inverse_transform(X_pca_outlier)\n",
        "\n",
        "reconstruction_errors = np.mean((X_scaled - X_reconstructed) ** 2, axis=1)\n",
        "mean_err = np.mean(reconstruction_errors)\n",
        "std_err = np.std(reconstruction_errors)\n",
        "threshold = mean_err + 3 * std_err\n",
        "\n",
        "valid_indices = np.where(reconstruction_errors <= threshold)[0]\n",
        "X_scaled_inliers = X_scaled[valid_indices]\n",
        "df_inliers = df_sample.iloc[valid_indices].copy()\n",
        "\n",
        "print(f\"Original sample size: {len(df_sample)}\")\n",
        "print(f\"Inliers after removing outliers: {len(df_inliers)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYQNMfa41cez"
      },
      "source": [
        "Step 6: Re-check the optimum number of clusters in the PCA-reduced space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tRJ0VP2M1dj0"
      },
      "outputs": [],
      "source": [
        "pca_for_clustering = PCA(n_components=n_components_needed, random_state=42)\n",
        "X_pca_for_clustering = pca_for_clustering.fit_transform(X_scaled)\n",
        "\n",
        "sil_scores = []\n",
        "distortions = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    km = KMeans(n_clusters=k, random_state=42)\n",
        "    km.fit(X_pca_for_clustering)\n",
        "    sil_scores.append(silhouette_score(X_pca_for_clustering, km.labels_))\n",
        "    distortions.append(km.inertia_)\n",
        "\n",
        "# Distortion (Elbow Method)\n",
        "plt.figure()\n",
        "plt.plot(K_range, distortions, 'o-')\n",
        "plt.title('Distortion (Elbow Method)')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.show()\n",
        "\n",
        "# Sihouette Score\n",
        "plt.figure()\n",
        "plt.plot(K_range, sil_scores, 'o-')\n",
        "plt.title('Silhouette Scores')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sppkEnVu1jd-"
      },
      "source": [
        "- **Elbow Method:** Distortion steadily declines with more clusters; a mild “elbow” appears around 5–6, indicating diminishing gains beyond that point.\n",
        "- **Silhouette Trend:** Scores rise sharply and peak near k=6, suggesting stronger intra-cluster cohesion and inter-cluster separation at that point.\n",
        "\n",
        "**Conclusion:** Combining both elbow and silhouette results, 6 clusters balance compactness and separation most effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V5i9PRO1mww"
      },
      "source": [
        "**Silhouette Visualizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u6vwufsg1gCf"
      },
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans_sil = KMeans(n_clusters=6, random_state=42)\n",
        "viz_data = X_scaled_inliers\n",
        "\n",
        "visualizer = SilhouetteVisualizer(kmeans_sil, colors='yellowbrick')\n",
        "visualizer.fit(viz_data)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkZ0Wbbb1rlw"
      },
      "source": [
        "**Silhouette Plot (k=6):** Average coefficient around 0.20, indicating moderate cluster distinctiveness; each cluster band shows decent separation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsM2kQ3o1uP-"
      },
      "source": [
        "Step 7: Fit final PCA + K-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "37CpxBHR1sI-"
      },
      "outputs": [],
      "source": [
        "optimal_k = 6\n",
        "pca_final = PCA(n_components=n_components_needed, random_state=42)\n",
        "X_pca_final = pca_final.fit_transform(X_scaled)\n",
        "\n",
        "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "kmeans_final.fit(X_pca_final)\n",
        "\n",
        "df_sample['cluster_label'] = kmeans_final.labels_\n",
        "\n",
        "# Map clusters back to the original DataFrame\n",
        "df['cluster_label'] = np.nan\n",
        "df.loc[df_sample.index, 'cluster_label'] = df_sample['cluster_label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdjD62sr1y4P"
      },
      "source": [
        "Step 8: Visualize the clusters in PCA space (using first two PCs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QbMVeR5M1w3i"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "sns.scatterplot(\n",
        "    x=X_pca_final[:, 0],\n",
        "    y=X_pca_final[:, 1],\n",
        "    hue=kmeans_final.labels_,\n",
        "    palette='Set2'\n",
        ")\n",
        "plt.title('Clusters in PCA Space (First Two Components)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7beFVxP-198"
      },
      "source": [
        "**Interpretation**\n",
        "\n",
        "- Six Clusters in Reduced Space: Each color represents a different cluster projected onto the first two principal components, showing how the data broadly separates into six groups.\n",
        "- Partial Overlaps: While some clusters appear well defined (pink at lower left, orange at upper right), others blend together, indicating boundary overlaps in 2D.\n",
        "- High-Level View: This 2D plot captures part of the data’s variance; clusters may be more distinct in the full PCA space (with all retained components)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM06_atn_CtC"
      },
      "source": [
        "Step 9: Visualize PCAs in 3D space using plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q0vn2XeR_FO_"
      },
      "outputs": [],
      "source": [
        "pca_3d = PCA(n_components=3, random_state=42)\n",
        "X_pca_3d = pca_3d.fit_transform(X_scaled_inliers)\n",
        "\n",
        "# Fit a KMeans\n",
        "kmeans_3d = KMeans(n_clusters=6, random_state=42)\n",
        "kmeans_3d.fit(X_pca_3d)\n",
        "labels_3d = kmeans_3d.labels_\n",
        "\n",
        "fig = px.scatter_3d(\n",
        "    x=X_pca_3d[:, 0],\n",
        "    y=X_pca_3d[:, 1],\n",
        "    z=X_pca_3d[:, 2],\n",
        "    color=labels_3d.astype(str),\n",
        "    title=\"3D Visualization of Clusters in PCA Space\"\n",
        ")\n",
        "fig.update_traces(marker=dict(size=3))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VAU2iUI_Lvq"
      },
      "source": [
        "**Interpretation**\n",
        "\n",
        "- Multi-Dimensional Representation: Each point is plotted in a 3D space of the first three principal components, providing a richer view of cluster separation than 2D.\n",
        "- Cluster Color-Coding: The differently colored points represent distinct K-means clusters, showing how samples group together or overlap in this reduced dimensional space.\n",
        "- Insight into Structure: Some clusters (red cluster near the top) separate more clearly, while others intermix, indicating that additional components or feature analysis may be needed to fully distinguish them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PJpe4KL_PFb"
      },
      "source": [
        "Step 10: Explore cluster statistics (mean values of each feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qidkI4Gu_RMF"
      },
      "outputs": [],
      "source": [
        "cluster_stats = df_sample.groupby('cluster_label').mean()\n",
        "display(cluster_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6xrxvFu_TiH"
      },
      "source": [
        "**Interpretation**\n",
        "\n",
        "- Mean Feature Profiles: Each row shows the average (scaled) feature values for a cluster, revealing how groups differ in attributes like latitude, complaint density, or resolution times.\n",
        "- Distinct Patterns: For instance, Cluster 1 has a markedly higher latitude and lower complaint_type_score than others, while Cluster 3 shows negative latitude and longitude values, hinting at distinct location-based behaviors.\n",
        "- Operational Insight: These patterns help identify the nature of each cluster (e.g., faster resolutions vs. higher complaint volumes) and guide targeted actions for resource allocation or process improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4ILJoTf_WLZ"
      },
      "source": [
        "Step 11: Interpret Principal Components (PC Loadings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2gIz0YfE_X-R"
      },
      "outputs": [],
      "source": [
        "loadings = pd.DataFrame(\n",
        "    pca_final.components_,\n",
        "    columns=df_numeric.columns[:],\n",
        "    index=[f'PC{i+1}' for i in range(n_components_needed)]\n",
        ")\n",
        "display(loadings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98AnM8is_aLI"
      },
      "source": [
        "**Interpretation**\n",
        "\n",
        "- Feature Contributions: Each row (PC1, PC2, etc.) shows how much each feature contributes (positively or negatively) to that principal component. Larger absolute values indicate a stronger influence on the PC.\n",
        "- Example : if latitude and resolution_time_minutes have large positive loadings on PC1, that component is partly describing how complaints vary by location and resolution speed.\n",
        "- Dimension Reduction Insight: These loadings help explain which original features most drive each principal component, offering clues about the underlying patterns captured after dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whcjq3NU_cgU"
      },
      "source": [
        "**Summary of K-Means Clustering**\n",
        "\n",
        "**Findings**\n",
        "\n",
        "1. Six Distinct Clusters: Varying in geographic location (latitude/longitude), complaint density, and resolution times, offering valuable segmentation for NYC 311 requests.\n",
        "2. Effective Dimension Reduction: About 7 principal components retained ~90% of data variance, allowing clear though not absolute separation in 2D/3D PCA plots.\n",
        "3. Cluster Statistics: Mean feature profiles reveal each group’s traits—some clusters have higher complaint density or faster resolution times, while others differ by geography.\n",
        "\n",
        "**Analysis**\n",
        "\n",
        "- Preprocessing & PCA: Data was sampled (5,000 rows), standardized, and reduced to the principal components that capture ~90% variance. Outlier removal helped refine clustering.\n",
        "- Optimal k Determination: The elbow method and silhouette scores both pointed to k=6 for the most balanced partition.\n",
        "- Visualization:\n",
        "  - 2D (PC1 vs. PC2): Shows broad cluster separation with moderate overlap.\n",
        "  - 3D View: Provides a richer perspective; certain clusters are more distinct, although some still partially intermix.\n",
        "  - Cluster Statistics: Aggregated means highlight how each cluster differs in latitude, complaint density, resolution times, etc.\n",
        "\n",
        "\n",
        "**Challenges**\n",
        "\n",
        "1. Cluster Shape Constraints: K-Means imposes roughly spherical clusters, which might not perfectly represent complex real-world data distributions.\n",
        "2. Scalability Trade-Off: Although K-Means scales better than Hierarchical Clustering, the large dataset still necessitated sampling to manage computation time.\n",
        "3. Interpretability: While PCA aids dimensionality reduction, interpreting principal components requires examining loadings to see which features drive each component.\n",
        "\n",
        "\n",
        "**Impact on NYC 311 Service Optimization**\n",
        "\n",
        "\n",
        "- Targeted Resource Deployment: Clusters reveal neighborhoods with consistently high complaint volumes, guiding where additional staff or resources should be allocated to cut resolution times.\n",
        "- Tailored Response Strategies: By examining each cluster’s unique features (e.g., high complaint_type_score, frequent weekend requests), the city can develop customized interventions for faster, more effective service.\n",
        "- Proactive Issue Management: Identifying emerging hotspots or seasonal spikes allows 311 authorities to address potential surges in requests before they escalate.\n",
        "\n",
        "\n",
        "**Recommendations for NYC 311 Authorities**\n",
        "\n",
        "1. Cluster-Specific Service Plans: For areas with persistently high complaint density, dedicate specialized teams or targeted initiatives (e.g., noise enforcement, sanitation sweeps) to reduce recurring issues.\n",
        "2. Data-Driven Policy: Continually analyze cluster patterns and incorporate new features—like complaint descriptions—to refine strategies, ensuring responses align with real-time needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImV3hio0Kyjt"
      },
      "source": [
        "## **3. Text Mining**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mpx0OsCp3ba"
      },
      "source": [
        "### **3.1 Bag of Words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHSCVVTjS7OS"
      },
      "source": [
        "**Goal:**\n",
        "The goal of the Bag of Words (BoW) analysis is to identify the most frequently occurring words in the \"resolution_description\" column, helping to uncover common themes within NYC 311 complaints. By converting text data into a structured format, BoW allows us to analyze patterns across different boroughs and complaint types, helping city officials and agencies better understand recurring issues.\n",
        "\n",
        "This step serves as the foundation for further text analysis by revealing high-frequency words that characterize different complaints. These words can highlight common resolution actions, frequent violations, or recurring issues across different city departments.\n",
        "\n",
        "**Application to 311 Data:**\n",
        "Understanding which words appear most frequently in complaint resolutions provides valuable insights into how issues are addressed and which problems occur most often. By grouping complaints by borough and type, we can determine:\n",
        "\n",
        "\n",
        "*   Which issues are most prevalent in certain areas.\n",
        "*   How different city agencies respond to complaints and whether certain types\n",
        "    of complaints are being addressed efficiently.\n",
        "*   Emerging patterns in complaints, which can inform proactive intervention\n",
        "    strategies.\n",
        "\n",
        "**Challenges:**\n",
        "Initially, extracting meaningful insights from BoW was difficult due to the large number of stop words in the dataset. Filtering out these common but unhelpful words was necessary to ensure that the analysis revealed meaningful insights to improve 311 operations.\n",
        "\n",
        "Additionally, BoW does not capture context, meaning further techniques like Topic Modeling and TF-IDF are required to uncover deeper relationships between words and complaint types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIbUosQGqGq9"
      },
      "source": [
        "Step 1: Text cleaning & regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LvP5rThCqCMW"
      },
      "outputs": [],
      "source": [
        "df_text = df[\"resolution_description\"]\n",
        "df_text.dropna()\n",
        "\n",
        "df_modified = df_text.copy()\n",
        "df_modified = pd.DataFrame(df_modified.str.lower()) # to lower case.\n",
        "df_modified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9PFuZvsrMcw"
      },
      "source": [
        "Step 2: Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LJq2hiSWrJsP"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nCOcTPU9rRyi"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize, WhitespaceTokenizer, RegexpTokenizer\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "import nltk\n",
        "df['resolution_description'] = df['resolution_description'].astype(str).fillna('')\n",
        "tokenized = [word_tokenize(t) for t in df['resolution_description']]\n",
        "tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V00lOxCxrWuq"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # Get the set of stop words\n",
        "tokenized_filtered = []\n",
        "for tokenized_sentence in tokenized:\n",
        "  tokenized_filtered.append([word for word in tokenized_sentence if word.lower() not in stop_words]) # filter them out\n",
        "\n",
        "tokenized_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khbZLJfOrcL_"
      },
      "source": [
        "Step 3: Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t_oJjKJ6rd6p"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "tokenized_filtered_stemmed = []\n",
        "for tokenized_sentence in tokenized_filtered:\n",
        "  tokenized_filtered_stemmed.append([ps.stem(word) for word in tokenized_sentence])\n",
        "\n",
        "tokenized_filtered_stemmed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m0QNF240rgDR"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "for tokenized_sentence in tokenized_filtered_stemmed:\n",
        "  print(TreebankWordDetokenizer().detokenize(tokenized_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b1zISuirh5Z"
      },
      "source": [
        "Step 4: Carry Out Bag of Words (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WqhHYtMSrjNE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i5XJwcGtrmns"
      },
      "outputs": [],
      "source": [
        "def tokenize_lemmatize(sentence):\n",
        "  tokens = word_tokenize(sentence) # tokenize\n",
        "  lemmatized_tokens = [ps.stem(word) for word in tokens] # lemmatize\n",
        "  return lemmatized_tokens\n",
        "\n",
        "#model\n",
        "cv = CountVectorizer(tokenizer=tokenize_lemmatize, stop_words='english')\n",
        "\n",
        "# fit\n",
        "df_text = df_text.fillna('')\n",
        "cv.fit(df_text.values)\n",
        "\n",
        "print('number of `tokens`', len(cv.vocabulary_))\n",
        "cv.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_yXiFmmIroMp"
      },
      "outputs": [],
      "source": [
        "dtm = cv.transform(df_text.values)\n",
        "bow = pd.DataFrame(dtm.toarray(), columns=cv.get_feature_names_out())\n",
        "bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-dE_igIurr8j"
      },
      "outputs": [],
      "source": [
        "recognized_tokens_sentence0 = cv.inverse_transform([bow.iloc[0]])\n",
        "recognized_tokens_sentence0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9efRTDuTQP_"
      },
      "source": [
        "Step 5: Find the Top 20 Words in the Resolution Descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3HYeyvWurxvc"
      },
      "outputs": [],
      "source": [
        "# Top 20 Words Used in Resolution Description\n",
        "\n",
        "# Sum word occurrences across all documents\n",
        "tfidf_model = TfidfVectorizer(stop_words=[\"the\", \"of\", \"to\", \"and\", \"is\", \"for\", \"this\", \"if\", \"that\", \"has\",\n",
        "                                          \"they\", \"these\", \"been\", \"at\", \"were\", \"was\"])\n",
        "X_bow_matrix = tfidf_model.fit_transform(df_text.values)\n",
        "feature_names = tfidf_model.get_feature_names_out()\n",
        "\n",
        "word_counts = X_bow_matrix.sum(axis=0).A1\n",
        "\n",
        "# Sort words by frequency\n",
        "top_n = 20\n",
        "top_indices = word_counts.argsort()[-top_n:][::-1]\n",
        "\n",
        "# Create DataFrame with words and their counts\n",
        "feature_names = tfidf_model.get_feature_names_out()\n",
        "\n",
        "top_words_df = pd.DataFrame({\n",
        "    \"word\": feature_names[top_indices],\n",
        "    \"count\": word_counts[top_indices]\n",
        "})\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words = \"english\")\n",
        "\n",
        "X = vectorizer.fit_transform(df_text.values)\n",
        "\n",
        "top_words_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WaryN_Har51y"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get word frequencies, ensuring values are numeric\n",
        "word_frequencies = {k: v for k, v in bow.sum().to_dict().items() if isinstance(v, (int, float))}\n",
        "\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_frequencies)\n",
        "\n",
        "# Display word cloud\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Most Frequent Words in Complaint Resolutions\", fontsize=14, fontweight=\"bold\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwEHulOHv6TN"
      },
      "source": [
        "**Word Cloud Interpretation:** The most frequent words in the complaint resolution descriptions include \"complaint,\" \"depart,\" \"preserv,\" \"inspect,\" \"respond,\" \"develop,\" and \"violation.\" This suggests that a large portion of the 311 resolution texts involve city departments addressing complaints through inspections, responses, and development-related actions. The frequent appearance of terms like \"police,\" \"housing,\" and \"condition\" indicates that many cases involve law enforcement, housing preservation, and environmental conditions.\n",
        "\n",
        "Additionally, words such as \"inform,\" \"close,\" and \"follow\" imply a structured resolution process where agencies provide information, follow up, and formally close complaints. The presence of words like \"correct,\" \"update,\" and \"protect\" suggests that resolutions often involve corrective measures and efforts to maintain compliance with city regulations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu0LJ5LpTeVG"
      },
      "source": [
        "Step 6: Top Words in Resolution Complaints by Borough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a9oXJgxdr-VC"
      },
      "outputs": [],
      "source": [
        "df_text = df_textmining[['borough', 'resolution_description']].copy()\n",
        "\n",
        "# Define key complaint words to analyze\n",
        "key_words = [\"complaint\", \"department\", \"police\", \"condition\", \"violations\", \"inspected\",\n",
        "             \"conditions\", \"housing\", \"preservation\", \"development\", \"issued\", \"closed\", \"available\",\n",
        "            \"request\", \"information\", \"action\"]\n",
        "\n",
        "# Count occurrences of key words in each borough\n",
        "word_counts_by_borough = pd.DataFrame(columns=key_words, index=df_text['borough'].unique())\n",
        "\n",
        "# Fill DataFrame with word occurrences\n",
        "for word in key_words:\n",
        "    word_counts_by_borough[word] = df_text.groupby('borough')['resolution_description'].apply(lambda x: x.str.contains(word, case=False, na=False).sum())\n",
        "\n",
        "word_counts_by_borough = word_counts_by_borough.fillna(0)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(word_counts_by_borough, annot=True, cmap=\"Blues\", fmt=\"g\", annot_kws={\"size\": 8})\n",
        "plt.title(\"Key Complaint Word Frequency by Borough\")\n",
        "plt.xlabel(\"Resolution Word\")\n",
        "plt.ylabel(\"Borough\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPyRqjp5wUje"
      },
      "source": [
        "**Analysis:** This heatmap highlights the distribution of key resolution words across NYC boroughs. \"Complaint,\" \"department,\" and \"police\" appear frequently, with Brooklyn and Queens showing the highest concentration, suggesting a larger volume of complaints requiring agency intervention. Housing-related terms like \"violations,\" \"inspected,\" and \"preservation\" are prominent, indicating that property issues and inspections are common. The word \"police\" is present across all boroughs, reflecting frequent law enforcement involvement in complaint resolutions.\n",
        "\n",
        "**Takeaway:** Brooklyn and Queens have the highest complaint volumes involving city departments, meaning that these areas must be heavily staffed and ready to respond. Housing violations and inspections are major resolution themes, suggesting that property management is prevalent throughout the whole city."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMyHOKFMTmjW"
      },
      "source": [
        "Step 7: Top Words in Resolution Description by Complaint Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0oHKtIEosM-F"
      },
      "outputs": [],
      "source": [
        "# Group by complaint type\n",
        "bow = bow.iloc[:len(df)]\n",
        "bow[\"complaint_type\"] = df[\"complaint_type\"].values\n",
        "\n",
        "grouped_bow = bow.groupby(\"complaint_type\").sum()\n",
        "\n",
        "top_n = 4\n",
        "\n",
        "# Function to extract meaningful words and ensure all columns are filled\n",
        "def extract_top_words(row):\n",
        "    words = row.nlargest(top_n).index.tolist()\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "    while len(words) < top_n:\n",
        "        words.append(\"N/A\")\n",
        "    return words\n",
        "\n",
        "# Apply function to get top words per complaint type\n",
        "top_words = grouped_bow.apply(extract_top_words, axis=1)\n",
        "\n",
        "# Convert to DataFrame\n",
        "top_words_df = pd.DataFrame(top_words.tolist(), index=top_words.index, columns=[f\"top_word_{i+1}\" for i in range(top_n)])\n",
        "top_words_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2HSlJIiByR9"
      },
      "source": [
        "Visualization by Complaint Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yNTATzapsUeO"
      },
      "outputs": [],
      "source": [
        "# Get the top 10 most frequent complaint types\n",
        "top_10_complaints = df[\"complaint_type\"].value_counts().nlargest(10).index\n",
        "\n",
        "# Filter dataset to only include top 10 complaints\n",
        "df_filtered = df[df[\"complaint_type\"].isin(top_10_complaints)]\n",
        "\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import string\n",
        "\n",
        "grouped_bow.columns = [word.translate(str.maketrans('', '', string.punctuation)) for word in grouped_bow.columns]\n",
        "word_totals = grouped_bow.sum().sort_values(ascending=False).head(10).index\n",
        "grouped_bow_filtered = grouped_bow.loc[top_10_complaints, word_totals]\n",
        "\n",
        "# Plot stacked bar chart\n",
        "grouped_bow_filtered.plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"Set2\")\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel(\"Complaint Type\")\n",
        "plt.ylabel(\"Word Count\")\n",
        "plt.title(\"Word Distribution Across Top 10 Complaint Types\")\n",
        "plt.legend(title=\"Top Words\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT7b7-76wmpj"
      },
      "source": [
        "**Analysis:** We see that Heating-related complaints have the highest word count, followed by Noise - Residential and Heat/Hot Water issues. Frequent words like “complaint,” “department,” and “police” indicate high agency involvement, while \"inspect,\" \"violate,\" and \"develop\" suggest a focus on compliance and enforcement.\n",
        "\n",
        "**Takeaway:** Heating complaints and noise-related issues dominate 311 resolutions. Housing and law enforcement terms appear across many complaint types, emphasizing ongoing oversight and quick response times to these prevalent issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPwM1LcTDf8l"
      },
      "source": [
        "**Summary of Bag of Words (BOW)**  \n",
        "\n",
        "- **Findings:** The most frequent words in 311 resolution descriptions include “complaint,” “department,” “police,” “inspect,” “violation,” and “develop”, indicating common themes of enforcement, response actions, and compliance. Heating, noise, and plumbing complaints generate the highest word counts in descriptions.  \n",
        "\n",
        "- **Analysis:** The dominance of words related to law enforcement and inspections suggests that a large portion of 311 cases require agency involvement, regulatory action, or follow-up investigations. Additionally, borough-level word frequency analysis highlights that Brooklyn and Queens have the highest complaint resolution activity.  \n",
        "\n",
        "- **Impact on 311 Service Optimization:** Understanding frequent resolution terms allows city agencies to better allocate resources, ensuring that departments like housing, sanitation, and law enforcement are **adequately staffed in high-complaint areas. Identifying recurring themes in resolution language can also help streamline response protocols.  \n",
        "\n",
        "- **Recommendations for 311:**  \n",
        "  1. Improve response efficiency by developing predefined action plans** for the most common complaint categories.  \n",
        "  2. Enhance transparency by standardizing resolution descriptions, making it easier to track service effectiveness.  \n",
        "  3. Optimize resource distribution by assigning more field agents and patrol cars to boroughs with higher complaint volumes.  \n",
        "\n",
        "- **Next Step – Topic Modeling:** To uncover deeper themes in complaint resolutions, we will apply Topic Modeling to group common words into meaningful categories. This will help identify patterns in complaint resolutions and guide improvements in NYC’s 311 service framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD1uM69UrsYr"
      },
      "source": [
        "### **3.2 Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mshqp3YWWh6S"
      },
      "source": [
        "**Goal:** With thousands of 311 calls made in NYC every day, it is essential to prioritize urgent requests and improve response times to ensure high-quality service for residents. For example, a gas leak requires immediate attention, whereas a noise complaint may not be as time-sensitive. To categorize different types of complaints efficiently, we can apply topic modeling to identify key groupings. This allows city agencies to quickly determine which department should respond to a complaint and whether it requires urgent action.\n",
        "\n",
        "**Application to 311 Data** By carrying out topic modeling first, we can split the data into distinct categories prior to clustering to reveal some common themes and similar resolution types.\n",
        "\n",
        "**Challenges:** The topic modeling method we learned in class (NMF) was incompatible with the dataset. After spending several hours troubleshooting errors, I researched alternative approaches and found that LDA was a better fit. It provided quick, interpretable results that aligned well with the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbpSOly1EG2W"
      },
      "source": [
        "Find the Number of Topics in Resolution Descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fV8lhuTMyQ5W"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X = vectorizer.fit_transform(df[\"resolution_description\"])\n",
        "\n",
        "# Apply LDA\n",
        "lda_model = LatentDirichletAllocation(n_components=4, random_state=42)\n",
        "lda_model.fit(X)\n",
        "\n",
        "# Print Top Words in Each Topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    print(f\"Topic {topic_idx + 1}: \", [feature_names[i] for i in topic.argsort()[-10:]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6x77Rpfyhqe"
      },
      "source": [
        "Topics Identified:\n",
        "\n",
        "1.   Topic 0: General\n",
        "2.   Topic 1: Transportation/Public Space\n",
        "3.   Topic 2: Emergency\n",
        "4.   Topic 3: Housing/Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T-hHjAnPQz0f"
      },
      "outputs": [],
      "source": [
        "# Dictionary mapping topic numbers to descriptions\n",
        "topic_labels = {\n",
        "    0: \"General Violaton\",\n",
        "    1: \"Transportation/Public Space Violation\",\n",
        "    2: \"Urgent Violation\",\n",
        "    3: \"Housing/Building Violations\"\n",
        "}\n",
        "\n",
        "# Function to get the topic for a given complaint index\n",
        "def get_complaint_topic(index):\n",
        "    complaint_type = df[\"complaint_type\"].iloc[index]\n",
        "    document_topic_dist = lda_model.transform(vectorizer.transform([df['resolution_description'].iloc[index]]))[0]\n",
        "    topic_index = document_topic_dist.argmax()\n",
        "    topic_name = topic_labels.get(topic_index, \"Unknown Topic\")\n",
        "    return f\"Complaint Type: {complaint_type} | Assigned Topic: {topic_name}\"\n",
        "\n",
        "# Example usage\n",
        "index = 10\n",
        "display(get_complaint_topic(index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0OII_y8wTCJu"
      },
      "outputs": [],
      "source": [
        "print(get_complaint_topic(100))\n",
        "print(get_complaint_topic(370))\n",
        "print(get_complaint_topic(600))\n",
        "print(get_complaint_topic(400))\n",
        "print(get_complaint_topic(750))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRPrKtrrPuwZ"
      },
      "source": [
        "**Summary of Topic Modeling**\n",
        "\n",
        "**Findings:** The resolution descriptions were broken down into 4 topics: General, Transportation/Public Space, Urgent, and Housing/Building\n",
        "\n",
        "**Analysis:** After experimenting with different numbers of topics, I found that\n",
        "four topics effectively segmented the complaint descriptions into distinct and meaningful categories. Each topic captures a unique subset of 311 complaints, reflecting broader trends in service requests.\n",
        "\n",
        "**Impact on 311 Service Optimization:** By automating topic assignment through modeling, we reduce manual effort in analyzing complaint descriptions and uncover hidden patterns in service requests. This insight can improve response prioritization, allowing NYC agencies to proactively address recurring or severe issues based on emerging trends.\n",
        "\n",
        "**Recommendations for 311:**\n",
        "-Violations that are labeled 'Urgent' should be taken care of before the other three topics. Since there is a very large call volume, 311 responders can prioritze emergency requests over less urgent ones to increase satisfaction and have a faster response time for more threatening calls.\n",
        "\n",
        "**Next Step:** We will use TF-IDF to cluster similar complaints based on key terms. This will help refine complaint categorization and allow for a more data-driven approach to understanding service request patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMKw3j3q911F"
      },
      "source": [
        "### **3.3 TF-IDF to Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUpxN8JxGcjF"
      },
      "source": [
        "**Goal:** The goal of TF-IDF is to find the most important words in the resolution description by converting the text to numbers. By weighting words based on their importance within individual complaints versus across the entire dataset, we aim to enhance the clustering process and uncover meaningful groupings.\n",
        "\n",
        "**Application to 311 Data:** We will use TF-IDF to see if it can label the keywords to be used in clustering for the different types of resolution descriptions. Since 311 complaints contain diverse and unstructured text, TF-IDF provides a structured way to analyze textual patterns and compare results with other text-mining techniques like topic modeling.\n",
        "\n",
        "**Challenges:** TF-IDF proved to create worse clusters than topic modeling, so we did not use the cluster results for further analysis. The clusters generated by TF-IDF lacked clear separations and often contained overlapping themes, making it difficult to derive actionable insights from the clustering output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_Qd0kp31WT"
      },
      "source": [
        "Clustering based on Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3-EyDpEUYjhb"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 4\n",
        "topic_distributions = lda_model.transform(vectorizer.transform(df[\"resolution_description\"]))\n",
        "topic_df = pd.DataFrame(topic_distributions, columns=[f\"Topic_{i}\" for i in range(num_clusters)])\n",
        "\n",
        "num_clusters = 4\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(topic_df)\n",
        "df[\"cluster\"] = cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fzc9sP42isoo"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=[col for col in df.columns if \"Topic_\" in col], errors=\"ignore\")  # Remove old topic columns\n",
        "df = pd.concat([df, topic_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K6R8d2QBeAJQ"
      },
      "outputs": [],
      "source": [
        "print(df[\"cluster\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J-FVZgYBgRy_"
      },
      "outputs": [],
      "source": [
        "topic_df.reset_index()\n",
        "cluster_topic_means = df.groupby(\"cluster\")[topic_df.columns].mean()\n",
        "print(cluster_topic_means)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhFn_3SFmHiS"
      },
      "source": [
        "Resolution Time by Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B7qI6hEkxy6-"
      },
      "outputs": [],
      "source": [
        "resolution_by_cluster = df.groupby(\"cluster\")[\"resolution_time_minutes\"].mean().sort_values()\n",
        "resolution_by_cluster.plot(kind=\"barh\", figsize=(10,6), colormap=\"Set2\")\n",
        "plt.xlabel(\"Average Resolution Time\")\n",
        "plt.ylabel(\"Cluster (Complaint Type\")\n",
        "plt.title(\"Average Resolution Time by Cluster\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kizYXmPDjDAA"
      },
      "source": [
        "**Analysis:** Based on the clustering results, we can see that each cluster is highly correlated with a specific topic. This reinforces that our topic modeling was correct and K-means is providing the same results. Cluster 0 is Housing/Building, Cluster 1 is General, Cluster 2 is Transportation/Public Space, and Cluster 3 is Urgent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPK0MMfwgI7D"
      },
      "source": [
        "Clustering based on TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pIdlOQlRruV6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D-HmW-SXrv_f"
      },
      "outputs": [],
      "source": [
        "tfidf_model = TfidfVectorizer(norm=None)\n",
        "\n",
        "# Select the text column(s) you want to use for TF-IDF\n",
        "text_data = df['resolution_description']\n",
        "\n",
        "# Fit the TF-IDF model on the text data\n",
        "tfidf_model.fit(text_data)\n",
        "\n",
        "# Transform the text data into TF-IDF vectors\n",
        "df_tfidf_transformed = tfidf_model.transform(text_data)\n",
        "\n",
        "# Create a DataFrame from the TF-IDF vectors\n",
        "tfidf_vectors = pd.DataFrame(df_tfidf_transformed.toarray(), columns=tfidf_model.get_feature_names_out())\n",
        "\n",
        "tfidf_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMbaG4Kx72t"
      },
      "source": [
        "Step 2: Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JZ8DQkWABQUK"
      },
      "outputs": [],
      "source": [
        "# Cluster Complaint Types\n",
        "X = vectorizer.fit_transform(df[\"resolution_description\"])\n",
        "\n",
        "# Apply K-Means clustering\n",
        "num_clusters = 4\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df[\"cluster\"] = kmeans.fit_predict(X)\n",
        "\n",
        "# View sample results\n",
        "print(df[[\"resolution_description\", \"cluster\"]].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JlybtY6xj5JG"
      },
      "outputs": [],
      "source": [
        "print(df[\"cluster\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZxF3Dp0ykAgk"
      },
      "outputs": [],
      "source": [
        "topic_df.reset_index()\n",
        "cluster_topic_means = df.groupby(\"cluster\")[topic_df.columns].mean()\n",
        "print(cluster_topic_means)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trgTp1-gkCxt"
      },
      "source": [
        "**Analysis:** Based on the unequal number of observations per cluster and the low correlation between a specific cluster and topic, this method did not work as well. Clustering based on topic modeling proved to be more accurate than TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeFiBrzix3zP"
      },
      "source": [
        "**Clustering Summary**\n",
        "- **Findings:** The clustering process grouped 311 complaints based on their topic distributions, effectively segmenting them into four distinct clusters. Each cluster is strongly associated with a dominant topic, reinforcing the themes identified during topic modeling.\n",
        "\n",
        "- **Analysis:** The results indicate that similar complaints tend to cluster together, meaning K-Means successfully identified meaningful patterns in the resolution descriptions. This confirms that topic modeling provided a strong foundation for clustering. However, the overlap in some topics suggests that additional tuning, such as adjusting the number of clusters, could improve separation.\n",
        "\n",
        "- **Impact on 311 Service Optimization:** By grouping complaints into clusters, NYC agencies can better allocate resources to address similar issues collectively rather than on a case-by-case basis. Identifying dominant topics within each cluster allows for more efficient complaint handling and policy adjustments.\n",
        "\n",
        "- **Recommendations for 311:** Agencies should use these clusters to prioritize response strategies, ensuring that departments specializing in specific complaint categories focus on their respective clusters.\n",
        "\n",
        "- **Next Step:** After clustering, the next step is sentiment analysis of the complaint resolutions. This will provide deeper insight into the effectiveness of different agencies' responses and where improvements are needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orIoIXUKsezr"
      },
      "source": [
        "### **3.4 Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKyTacSeQGEu"
      },
      "source": [
        "**Goal:**\n",
        "The goal of Sentiment Analysis is to classify resolution descriptions as either positive or negative, with positive resolutions often associated with faster response times. To achieve this, we will develop an unsupervised model that assigns a sentiment label to each resolution description.\n",
        "\n",
        "**Application to 311 Data**\n",
        "Sentiment analysis on 311 resolution descriptions can provide insights into how effectively city services respond to citizen complaints. While we lack direct citizen feedback, analyzing officer-reported resolution descriptions allows us to infer whether a case was likely resolved efficiently and positively or faced challenges. Faster resolution times and reassuring language in reports (e.g., \"issue resolved promptly\") may indicate a positive sentiment, while delays or unresolved cases (e.g., \"further investigation required\") might suggest a negative sentiment.\n",
        "\n",
        "**Challenges:**\n",
        "A more insightful analysis would involve conducting sentiment analysis on citizen statements regarding their 311 call experience. This would allow us to determine whether residents were satisfied or dissatisfied with response times and case resolutions. However, since such data is unavailable, our analysis is limited to the official resolution descriptions provided by officers handling each case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cqqhLBtsq3r"
      },
      "source": [
        "Step 1: Cleanup Text & Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uijih6B5sljb"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def cleanup_text(sentence):\n",
        "  # First, word tokenize.\n",
        "  tokenized_sms_messages = word_tokenize(sentence)\n",
        "\n",
        "  # Lower case\n",
        "  tokenized_sms_messages = [word.lower() for word in tokenized_sms_messages]\n",
        "\n",
        "  # Remove punctuation\n",
        "  tokenized_sms_messages = [word for word in tokenized_sms_messages if word not in string.punctuation]\n",
        "\n",
        "  # Remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokenized_sms_messages = [word for word in tokenized_sms_messages if word not in stop_words]\n",
        "\n",
        "  # Stem\n",
        "  # tokenized_sms_messages = [ps.stem(word) for word in tokenized_sms_messages]\n",
        "\n",
        "  return tokenized_sms_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZBr0EAvszzV"
      },
      "source": [
        "Step 2: Sentiment Analysis by Average Resolution Time per Complaint Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GzeeTerOst_T"
      },
      "outputs": [],
      "source": [
        "# Calculate resolution time in hours\n",
        "df['resolution_time'] = (df['closed_date'] - df['created_date']).dt.total_seconds() / 3600\n",
        "\n",
        "# Drop negative resolution times\n",
        "df = df[df['resolution_time'] >= 0]\n",
        "\n",
        "df['resolution_time'].mean()\n",
        "\n",
        "avg_res_time_by_complaint = df.groupby('complaint_type')['resolution_time'].mean()\n",
        "print(avg_res_time_by_complaint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg519QaZtBIX"
      },
      "source": [
        "Step 3: Assign Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sk_p4EKBs-7F"
      },
      "outputs": [],
      "source": [
        "def assign_sentiment(row):\n",
        "  complaint_type = row['complaint_type']\n",
        "  resolution_time = row['resolution_time']\n",
        "  average_resolution_time = avg_res_time_by_complaint[complaint_type]\n",
        "  if resolution_time <= average_resolution_time:\n",
        "    return \"positive\"\n",
        "  else:\n",
        "    return \"negative\"\n",
        "df['sentiment'] = df.apply(assign_sentiment, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbnp6nlItINw"
      },
      "source": [
        "Step 4: Create Our Own Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vxpHqCaCtCgF"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the pretrained model\n",
        "pretrained_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "vector_size = pretrained_model.vector_size  # Get the embedding size\n",
        "\n",
        "tokenized_reviews = df[\"resolution_description\"].apply(cleanup_text)\n",
        "\n",
        "embeddings = list(map(lambda tokenized_review: pretrained_model.get_mean_vector(tokenized_review) if len(tokenized_review) > 0 else np.zeros(vector_size), tokenized_reviews))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z3H4W4xdtE_H"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "def assess_model(df, X, y_column):\n",
        "  # train/test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, df[y_column], test_size=0.2, random_state=42)\n",
        "\n",
        "  # train the model\n",
        "  classifier = KNeighborsClassifier()\n",
        "  classifier.fit(X_train, y_train)\n",
        "\n",
        "  # Predict on the test data\n",
        "  y_pred = classifier.predict(X_test)\n",
        "\n",
        "  # Evaluate the model\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  f1_score = sklearn.metrics.f1_score(y_test, y_pred, pos_label=\"positive\")\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  print(f\"f1_score: {f1_score}\")\n",
        "  print(sklearn.metrics.classification_report(y_test,y_pred))\n",
        "  display(pd.DataFrame(confusion_matrix(y_test, y_pred, normalize='true'), columns=classifier.classes_, index=classifier.classes_ ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vHBoLyjrtQG-"
      },
      "outputs": [],
      "source": [
        "assess_model(df, embeddings, \"sentiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK8CQr3fIJW0"
      },
      "source": [
        "**Check the logic of our model:** It seems like complaints that had a clean resolution have been labeled positive, and ones that needed further attention and multiple actions have been labeled negative. It also correlates with low vs high resolution times. Our model is appearing to be working correctly. There is room for error here because we do not have all of the context and we do not have input from citizens to model. Note that the model is biased towards labeling resolution descriptions positive, so we must take this into account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LknXwqKEIMFp"
      },
      "outputs": [],
      "source": [
        "df_sentiment = df[['resolution_description', 'sentiment', 'resolution_time']]\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df_sentiment.iloc[[30, 31, 10, 23, 39]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp3j-Fc6tXyI"
      },
      "source": [
        "Visualize Sentiment Analysis by Month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a_1Hr1bstVwm"
      },
      "outputs": [],
      "source": [
        "import calendar\n",
        "# Convert month name to categorical with correct order (Jan-Dec)\n",
        "df['created_month'] = pd.to_datetime(df['created_date']).dt.month\n",
        "df['created_month'] = df['created_month'].apply(lambda x: calendar.month_abbr[x])\n",
        "\n",
        "# Calculate positive sentiment proportion\n",
        "df['sentiment_num'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "monthly_sentiment = df.groupby(\"created_month\")['sentiment_num'].mean().reset_index()\n",
        "\n",
        "# Sort by month order (Jan-Dec)\n",
        "month_order = list(calendar.month_abbr)[1:]\n",
        "monthly_sentiment['created_month'] = pd.Categorical(monthly_sentiment['created_month'], categories=month_order, ordered=True)\n",
        "monthly_sentiment = monthly_sentiment.sort_values(\"created_month\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_sentiment[\"created_month\"], monthly_sentiment[\"sentiment_num\"], marker=\"o\", linestyle=\"-\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Proportion of Positive Sentiment\")\n",
        "plt.title(\"Sentiment Trends by Month\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnUNg9FeI2rn"
      },
      "source": [
        "**Interpretation:** The line chart shows sentiment trends in NYC 311 complaints resolutions by month, with the proportion of positive sentiment fluctuating by month. The sentiment score starts relatively low in January and increases sharply from February to April, peaking around April and May. This suggests that complaint resolutions during these months may have been handled more effectively or that complaints were less severe. However, from June onward, sentiment gradually declines, with a steep drop from September to November, reaching the lowest sentiment proportion in November. This could indicate increased dissatisfaction due to seasonal factors, policy changes, or resource constraints in handling complaints. The slight rebound in December suggests some recovery in sentiment, but it remains lower than mid-year levels.\n",
        "\n",
        "**Takeaway:** Understanding these trends can help NYC agencies allocate resources more effectively during months of lower public satisfaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv_ektdJCKjm"
      },
      "source": [
        "Visualize Sentiment Analysis by Year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fFyVDdi9B9dh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'created_date' to datetime format\n",
        "df['created_year'] = pd.to_datetime(df['created_date']).dt.year\n",
        "\n",
        "# Convert sentiment to numeric values (1 for positive, 0 for negative)\n",
        "df['sentiment_num'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Group by year and calculate the proportion of positive sentiment\n",
        "yearly_sentiment = df.groupby(\"created_year\")['sentiment_num'].mean().reset_index()\n",
        "\n",
        "# Plot sentiment trends by year\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(yearly_sentiment[\"created_year\"], yearly_sentiment[\"sentiment_num\"], marker=\"o\", linestyle=\"-\")\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Proportion of Positive Sentiment\")\n",
        "plt.title(\"Sentiment Trends by Year\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T2tDFiiJKcG"
      },
      "source": [
        "**Interpretation:** The line chart illustrates trends in NYC 311 complaints over time, showing fluctuations in complaint volume between 2010 and 2015. There are periods of high complaint activity, particularly before 2013, followed by a sharp drop, possibly due to a data reporting change or a temporary decline in service requests. After 2013, complaints steadily increase again, peaking around 2015, indicating a growing demand for city services.\n",
        "\n",
        "**Takeaway:** We need to assess trends post-2013 to understand what the 311 agencies improved and how they got the sentiment to increase so dramatically. Figuring out what works to have the fastest resolution times possible will be helpful in being consistent and ready."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjUsmTXMDhpy"
      },
      "source": [
        "Sentiment Analysis Summary\n",
        "\n",
        "**Findings:** Sentiment Analysis revealed varying levels of public satisfaction across different complaint types and time periods. Certain complaint categories, such as Illegal Fireworks and Taxi Reports, were associated with more negative sentiment, while DOF Property - Property Value and DOF Parking - Request Status had higher sentiment scores, suggesting a better resolution experience. Monthly sentiment trends showed an increase in positive sentiment from February to May, followed by a gradual decline, with November having the lowest sentiment scores.\n",
        "\n",
        "**Analysis:** The sentiment trends indicate that seasonality, complaint type, and resolution efficiency play key roles in shaping public perception. The surge in positive sentiment during the spring months could be due to increased city responsiveness or less severe complaints. In contrast, the drop in sentiment toward the end of the year may be linked to resource constraints, holiday-related complaints, or service delays. The clustering analysis further supports that some complaints consistently generate dissatisfaction, signaling areas where agencies can improve their approach.\n",
        "\n",
        "**Impact on 311 Service Optimization:** By understanding sentiment patterns, NYC agencies can prioritize complaints that lead to lower public satisfaction and allocate resources more efficiently. Identifying trends in negative sentiment can help reduce service bottlenecks, leading to faster resolutions and improved public trust. Seasonal sentiment fluctuations highlight the need for proactive service planning, ensuring that high-volume complaint periods do not result in declining sentiment.\n",
        "\n",
        "**Recommendations for 311:**\n",
        "311 agencies should prioritize complaints with low sentiment scores by focusing on improving resolution strategies for complaint types that generate the most dissatisfaction. They can also incorporate sentiment-based performance metrics into service quality reviews to track which agencies or complaint types need improvement. Agencies responding to 311 calls should have an efficient means of communication to ensure that all resolutions include clear and helpful updates to complainants.\n",
        "\n",
        "**Next Steps:** A great next step would be to develop a predictive model to anticipate which complaints will generate the most dissatisfaction and optimize response efforts accordingly. We could also get data on citizen sentiment based on how the complaint was handled to analyze public satisfaction directly. The models presented above are very uncertain because the resolution description was written by the 311 agency, not the citizen. Due to time constraints and potential redundancy, we did not carry out cosine similarity but we could definitely do that to see any trends that we missed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7mmFojRL6jx"
      },
      "source": [
        "# **Summary of Findings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyRr4Mx9fh95"
      },
      "source": [
        "Our analysis of NYC 311 service requests identified key patterns in complaint distribution, response times, and agency efficiency. Clustering methods revealed that high-density complaint areas are concentrated in Brooklyn, Manhattan, and the Bronx, indicating where city services are most needed. Temporal trends showed that complaints peak on Tuesdays, with midnight spikes primarily due to noise-related issues.\n",
        "\n",
        "Association Rule Mining uncovered strong geographic and agency-based relationships, with noise complaints prevalent in Manhattan and Brooklyn on weekends, parking violations concentrated in Queens and Brooklyn, and heat complaints peaking in the Bronx and Brooklyn during winter months. These insights support targeted interventions such as seasonal resource allocation and optimized enforcement strategies.\n",
        "\n",
        "Text mining techniques helped categorize resolution descriptions, identifying four key complaint topics: General, Public Space, Urgent, and Housing. Sentiment analysis revealed a dip in positive sentiment in 2013 and more negative sentiment during winter months, suggesting seasonal service inefficiencies.\n",
        "\n",
        "From a practical standpoint, these findings can enhance resource allocation, customer service efficiency, and emergency response planning. By optimizing urban service distribution, automating complaint categorization, and predicting high-demand periods, NYC agencies can streamline operations and improve citizen satisfaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgEQSkzCMFO2"
      },
      "source": [
        "# **Conclusion & Next Steps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0YSPXSSlOwq"
      },
      "source": [
        "Our analysis of NYC 311 service requests uncovered key insights that can enhance city resource allocation and service efficiency. Using association rule mining, we identified strong correlations between complaint types, boroughs, and resolution times, revealing borough-specific service demands such as noise complaints in Manhattan and Brooklyn and parking violations in Queens. Clustering analysis highlighted high-density complaint areas in Brooklyn, the Bronx, and Manhattan, enabling targeted interventions and faster response times. Text mining and sentiment analysis provided further insights into resolution descriptions, identifying four key complaint themes (public space, housing, urgent issues, general complaints) and sentiment trends that suggest longer resolution times correlate with negative sentiment, particularly in winter months.\n",
        "\n",
        "**Recommendations**\n",
        "\n",
        "* Optimized Resource Allocation – Deploy more enforcement for noise complaints on weekends and prioritize heating issue resolutions in winter.\n",
        "\n",
        "* Improved Response Strategies – Borough-specific complaint trends should guide staffing, such as increasing NYPD presence in parking-heavy areas or boosting heating specialists in the Bronx.\n",
        "\n",
        "* Better Citizen Engagement – Educate residents on proper agency reporting to reduce misdirected complaints and enhance resolution speed.\n",
        "\n",
        "* Automated Complaint Categorization – Implement NLP-driven classification to prioritize urgent requests in real-time, ensuring faster response times.\n",
        "\n",
        "* Predictive Modeling for City Services – Use historical trends to forecast service demands, allowing proactive deployment of city resources.\n",
        "\n",
        "**Future Steps**\n",
        "\n",
        "* Expand Data Scope – Incorporate additional citizen feedback sources to refine sentiment analysis.\n",
        "\n",
        "* Enhance Clustering Accuracy – Implement density-based clustering (DBSCAN) for better handling of non-spherical clusters.\n",
        "\n",
        "* Real-Time Monitoring Dashboard – Develop a live dashboard for city agencies to track complaints dynamically and optimize response efforts.\n",
        "\n",
        "* Deep Learning for Text Analysis – Explore BERT or LSTMs to improve the accuracy of complaint classification and sentiment prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBNVU-xnMIS-"
      },
      "source": [
        "### **Challenges**  \n",
        "\n",
        "Our analysis encountered multiple challenges across **association rule mining, clustering, and text analysis**, requiring adjustments to our methodologies. Issues such as **low lift values, computational inefficiencies, and data preprocessing difficulties** had to be addressed through alternative approaches and optimizations.  \n",
        "\n",
        "#### **Association Rule Mining**  \n",
        "- **Challenge:** Many rules had **low lift values**, making them weak despite high confidence.  \n",
        "  **Solution:** Adjusted **support and confidence thresholds** to filter out statistically insignificant rules.  \n",
        "- **Challenge:** High number of **unique complaint types** led to complex and low-support patterns.  \n",
        "  **Solution:** Grouped similar complaints to improve rule significance.  \n",
        "- **Challenge:** Ensuring correct data types and transaction encoding.  \n",
        "  **Solution:** Converted categorical variables and ensured proper encoding formats.  \n",
        "\n",
        "#### **Scalability and Clustering Challenges**  \n",
        "- **Challenge:** **Hierarchical clustering** was infeasible due to high memory usage and an uninterpretable dendrogram.  \n",
        "  **Solution:** Sampled **5,000 rows** and used **Ward’s method** to extract spatial insights.  \n",
        "- **Challenge:** **K-means clustering** struggled with sensitivity to initialization and non-spherical clusters.  \n",
        "  **Solution:** Applied **PCA for dimensionality reduction** and optimized k using the **elbow method**.  \n",
        "\n",
        "#### **Text Analysis and Topic Modeling**  \n",
        "- **Challenge:** **Bag of Words (BoW)** yielded weak insights due to excessive stop words.  \n",
        "  **Solution:** Applied **stop word filtering** and enhanced analysis with **Topic Modeling (LDA)**.  \n",
        "- **Challenge:** **NMF topic modeling** was incompatible with the dataset.  \n",
        "  **Solution:** Switched to **LDA**, which produced clearer and more interpretable results.  \n",
        "- **Challenge:** **Sentiment analysis** could have provided deeper insights but was limited by available data.  \n",
        "  **Solution:** Focused on official resolution descriptions to extract meaningful trends.  \n",
        "\n",
        "Despite overcoming many challenges, some methods like **hierarchical clustering on the full dataset and sentiment analysis on citizen feedback** remained infeasible due to computational and data constraints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnEB7ybBkj54"
      },
      "source": [
        "# **References**\n",
        "\n",
        "1. [Apriori Documentation](https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)\n",
        "2. [Mlxtend](https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/)\n",
        "3. [Justice Data Brief: Understanding New York City's 311 Data](https://datacollaborativeforjustice.org/wp-content/uploads/2019/03/DCJ-Justice-Data-Brief-NYC-311-Calls.pdf)\n",
        "4. [The value of 311 data - Hunter Urban Review](https://hunterurbanreview.commons.gc.cuny.edu/the-value-of-311-data/)\n",
        "5. [Harvard Univeristy : Transforming Municipal Customer Service in Chicago](https://datasmart.hks.harvard.edu/news/article/beyond-311-368)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLocnJwqDVfr"
      },
      "source": [
        "# **GenAI Undertaking**\n",
        "\n",
        "In this project, we used generative AI tools to assist with various aspects of our work. Below is a detailed account of how these tools were\n",
        "used:\n",
        "* **Code Review and Debugging**: We used GPT-4o to make our code more efficient, resolve syntax errors and add few components to certain\n",
        "visualisation codes.\n",
        "* **Brainstorming ideas**: We utilized Generative AI to brainstorm ideas and refine our approach for association rule mining, clustering, and text analysis in this project.\n",
        "* **Proofreading and Grammar Checks**: We used Grammarly to refine our writing, improve readability, and ensure grammatical accuracy.\n",
        "\n",
        "\n",
        "Our team has adhered to academic integrity standards throughout the process, reviewing and validating all AI-generated content to ensure its\n",
        "accuracy.\n",
        "\n",
        "**Links to Chats:**\n",
        "1. https://chatgpt.com/share/67c49e76-b14c-8004-bbc1-06b8579c7658\n",
        "2. https://chatgpt.com/share/67c49efc-023c-8004-8e17-43ec2d9c7883\n",
        "3. https://chatgpt.com/share/67c4a021-2e5c-8004-b2e8-f6c0786c62fa\n",
        "4. https://chatgpt.com/share/67c62306-e070-800b-b67d-fc1da2c89ee2\n",
        "5. https://docs.google.com/document/d/1Ay2jE9h5h30JD2TW6I-fx2JDx-dhOcGu8wwZ_GDVKSQ/edit?tab=t.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MZjo_becGvLa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wUBrivBaGzz7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!jupyter nbconvert --to PDF \"/content/drive/My Drive/BA_820_Project\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}